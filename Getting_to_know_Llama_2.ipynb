{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d680961880222a81",
   "metadata": {},
   "source": [
    "# **Getting to know Llama 2: Everything you need to start building**\n",
    "Our goal in this session is to provide a guided tour of Llama 2, including understanding different Llama 2 models, how and where to access them, Generative AI and Chatbot architectures, prompt engineering, RAG (Retrieval Augmented Generation), Fine-tuning and more. All this is implemented with a starter code for you to take it and use it in your Llama 2 projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545e17b77f35ab",
   "metadata": {},
   "source": [
    "## **0 - Prerequisites**\n",
    "* Basic understanding of Large Language Models\n",
    "* Basic understanding of Python"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:30.599183Z",
     "start_time": "2024-04-30T11:30:30.593389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%html\n",
    "<style>\n",
    "    .LLM {\n",
    "        background-color: skyblue;\n",
    "        font-family: D2Coding, Consolas, Arial, sans-serif;\n",
    "        font-size: 12pt;\n",
    "        padding: 10px;\n",
    "        margin: 10px;\n",
    "    }\n",
    "</style>"
   ],
   "id": "e22e289078070a39",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    .LLM {\n",
       "        background-color: skyblue;\n",
       "        font-family: D2Coding, Consolas, Arial, sans-serif;\n",
       "        font-size: 12pt;\n",
       "        padding: 10px;\n",
       "        margin: 10px;\n",
       "    }\n",
       "</style>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d87dc3a5a98e8f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:31.285160Z",
     "start_time": "2024-04-30T11:30:30.601162Z"
    }
   },
   "source": [
    "import base64\n",
    "from getpass import getpass\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image, Markdown, HTML\n",
    "\n",
    "from chrisbase.data import *\n",
    "from chrisbase.io import *\n",
    "from chrisbase.util import *\n",
    "\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "\n",
    "def md(t):\n",
    "    display(Markdown(t))\n",
    "\n",
    "\n",
    "def html(t):\n",
    "    display(HTML(t))\n",
    "\n",
    "\n",
    "def genai_app_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[Users] --> B(Applications e.g. mobile, web)\n",
    "        B --> |Hosted API|C(Platforms e.g. Custom, HuggingFace, Replicate)\n",
    "        B -- optional --> E(Frameworks e.g. LangChain)\n",
    "        C-->|User Input|D[Llama 2]\n",
    "        D-->|Model Output|C\n",
    "        E --> C\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def rag_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[User Prompts] --> B(Frameworks e.g. LangChain)\n",
    "        B <--> |Database, Docs, XLS|C[fa:fa-database External Data]\n",
    "        B -->|API|D[Llama 2]\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "\n",
    "def llama2_family():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        llama-2 --> llama-2-7b\n",
    "        llama-2 --> llama-2-13b\n",
    "        llama-2 --> llama-2-70b\n",
    "        llama-2-7b --> llama-2-7b-chat\n",
    "        llama-2-13b --> llama-2-13b-chat\n",
    "        llama-2-70b --> llama-2-70b-chat\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def apps_and_llms():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        users --> apps\n",
    "        apps --> frameworks\n",
    "        frameworks --> platforms\n",
    "        platforms --> Llama 2\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# Create a text widget\n",
    "API_KEY = widgets.Password(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='API_KEY:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "def bot_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        user --> prompt\n",
    "        prompt --> i_safety\n",
    "        i_safety --> context\n",
    "        context --> Llama_2\n",
    "        Llama_2 --> output\n",
    "        output --> o_safety\n",
    "        i_safety --> memory\n",
    "        o_safety --> memory\n",
    "        memory --> context\n",
    "        o_safety --> user\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def fine_tuned_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        Custom_Dataset --> Pre-trained_Llama\n",
    "        Pre-trained_Llama --> Fine-tuned_Llama\n",
    "        Fine-tuned_Llama --> RLHF\n",
    "        RLHF --> |Loss:Cross-Entropy|Fine-tuned_Llama\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def load_data_faiss_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        documents --> textsplitter\n",
    "        textsplitter --> embeddings\n",
    "        embeddings --> vectorstore\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def mem_context():\n",
    "    mm(\"\"\"\n",
    "    graph LR\n",
    "        context(text)\n",
    "        user_prompt --> context\n",
    "        instruction --> context\n",
    "        examples --> context\n",
    "        memory --> context\n",
    "        context --> tokenizer\n",
    "        tokenizer --> embeddings\n",
    "        embeddings --> LLM\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "acd794f15ce1f743",
   "metadata": {},
   "source": [
    "## **1 - Understanding Llama 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548692b6a6dd66c",
   "metadata": {},
   "source": [
    "### **1.1 - What is Llama 2?**\n",
    "* State of the art (SOTA), Open Source LLM\n",
    "* 7B, 13B, 70B\n",
    "* Pretrained + Chat\n",
    "* Choosing model: Size, Quality, Cost, Speed\n",
    "* [Research paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "* [Responsible use guide](https://ai.meta.com/llama/responsible-use-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7d7c15448e3f170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:31.288304Z",
     "start_time": "2024-04-30T11:30:31.286164Z"
    }
   },
   "source": [
    "llama2_family()"
   ],
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTdiCiAgICAgICAgbGxhbWEtMiAtLT4gbGxhbWEtMi0xM2IKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTcwYgogICAgICAgIGxsYW1hLTItN2IgLS0+IGxsYW1hLTItN2ItY2hhdAogICAgICAgIGxsYW1hLTItMTNiIC0tPiBsbGFtYS0yLTEzYi1jaGF0CiAgICAgICAgbGxhbWEtMi03MGIgLS0+IGxsYW1hLTItNzBiLWNoYXQKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "190b25e8cdc4c64d",
   "metadata": {},
   "source": [
    "### **1.2 - Accessing Llama 2**\n",
    "* Download + Self Host (on-premise)\n",
    "* Hosted API Platform (e.g. [Replicate](https://replicate.com/meta))\n",
    "* Hosted Container Platform (e.g. [Azure](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-llama-2-on-azure/ba-p/3881233), [AWS](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/), [GCP](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0f1cf7c500be4",
   "metadata": {},
   "source": [
    "### **1.3 - Use Cases of Llama 2**\n",
    "* Content Generation\n",
    "* Chatbots\n",
    "* Summarization\n",
    "* Programming (e.g. Code Llama)\n",
    "* and many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965614383680fb8",
   "metadata": {},
   "source": [
    "## **2 - Using Llama 2**\n",
    "In this notebook, we are going to access [Llama 13b chat model](https://replicate.com/meta/llama-2-13b-chat) using hosted API from Replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1a112c1342b73",
   "metadata": {},
   "source": [
    "### **2.1 - Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a77a8bb71e58ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:31.864561Z",
     "start_time": "2024-04-30T11:30:31.288845Z"
    }
   },
   "source": [
    "# Install dependencies and initialize\n",
    "!pip list | grep -E \"torch|transformers|chris|langchain|replicate|faiss|jupyter\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrisbase                 0.5.2\r\n",
      "faiss-cpu                 1.8.0\r\n",
      "jupyter                   1.0.0\r\n",
      "jupyter_client            8.6.1\r\n",
      "jupyter-console           6.6.3\r\n",
      "jupyter_core              5.7.2\r\n",
      "jupyter-events            0.10.0\r\n",
      "jupyter-lsp               2.2.5\r\n",
      "jupyter_server            2.14.0\r\n",
      "jupyter_server_terminals  0.5.3\r\n",
      "jupyterlab                4.1.8\r\n",
      "jupyterlab_pygments       0.3.0\r\n",
      "jupyterlab_server         2.27.1\r\n",
      "jupyterlab_widgets        3.0.10\r\n",
      "langchain                 0.1.16\r\n",
      "langchain-community       0.0.34\r\n",
      "langchain-core            0.1.46\r\n",
      "langchain-text-splitters  0.0.1\r\n",
      "replicate                 0.25.2\r\n",
      "sentence-transformers     2.7.0\r\n",
      "torch                     2.3.0\r\n",
      "transformers              4.40.1\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "8068c18c446acaec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:31.868757Z",
     "start_time": "2024-04-30T11:30:31.866599Z"
    }
   },
   "source": [
    "# model url on Replicate platform that we will use for inferencing\n",
    "# We will use llama 2 13b chat model hosted on replicate server ()agent_name = \"llama-2-13b-chat\"\n",
    "model_ref = f\"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "45ec8c200c5b6f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:31.889668Z",
     "start_time": "2024-04-30T11:30:31.869520Z"
    }
   },
   "source": [
    "# We will use Replicate hosted cloud environment\n",
    "# Obtain Replicate API key → https://replicate.com/account/api-tokens)\n",
    "\n",
    "# enter your replicate api token\n",
    "REPLICATE_API_TOKEN = read_or(first_path_or(\".replicate*\")) or getpass()\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "\n",
    "logging.getLogger(\"IPKernelApp\").setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "args = CommonArguments(\n",
    "    env=ProjectEnv(\n",
    "        project=\"LLM-based\",\n",
    "        job_name=\"Llama 2 13b Chat\",\n",
    "        msg_level=logging.INFO,\n",
    "        msg_format=LoggingFormat.BRIEF_00,\n",
    "    )\n",
    ")\n",
    "args.dataframe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      CommonArguments                                                                                        value\n",
       "0                 tag                                                                                         None\n",
       "1         env.project                                                                                    LLM-based\n",
       "2        env.job_name                                                                             Llama 2 13b Chat\n",
       "3     env.job_version                                                                                         None\n",
       "4        env.hostname                                                                           ChrisBookPro.local\n",
       "5        env.hostaddr                                                                                  192.168.0.8\n",
       "6      env.time_stamp                                                                                  0430.203031\n",
       "7     env.python_path                                        /Users/chris/miniforge3/envs/LLM-based/bin/python3.11\n",
       "8     env.current_dir                                                                  /Users/chris/proj/LLM-based\n",
       "9    env.current_file                                    /Users/chris/proj/LLM-based/Getting_to_know_Llama_2.ipynb\n",
       "10    env.working_dir                                                                  /Users/chris/proj/LLM-based\n",
       "11   env.command_args  [-f, /Users/chris/Library/Jupyter/runtime/kernel-5b304b7c-0257-40f7-9284-9c4ecb174507.json]\n",
       "12   env.num_ip_addrs                                                                                            0\n",
       "13    env.max_workers                                                                                            1\n",
       "14      env.debugging                                                                                        False\n",
       "15      env.msg_level                                                                                           20\n",
       "16     env.msg_format                                                                    %(asctime)s ┇ %(message)s\n",
       "17    env.date_format                                                                             [%m.%d %H:%M:%S]\n",
       "18    env.output_home                                                                                         None\n",
       "19   env.logging_file                                                                                         None\n",
       "20  env.argument_file                                                                                         None\n",
       "21       time.started                                                                                         None\n",
       "22       time.settled                                                                                         None\n",
       "23       time.elapsed                                                                                         None"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommonArguments</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tag</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>env.project</td>\n",
       "      <td>LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>env.job_name</td>\n",
       "      <td>Llama 2 13b Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>env.job_version</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>env.hostname</td>\n",
       "      <td>ChrisBookPro.local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>env.hostaddr</td>\n",
       "      <td>192.168.0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>env.time_stamp</td>\n",
       "      <td>0430.203031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>env.python_path</td>\n",
       "      <td>/Users/chris/miniforge3/envs/LLM-based/bin/python3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>env.current_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>env.current_file</td>\n",
       "      <td>/Users/chris/proj/LLM-based/Getting_to_know_Llama_2.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>env.working_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>env.command_args</td>\n",
       "      <td>[-f, /Users/chris/Library/Jupyter/runtime/kernel-5b304b7c-0257-40f7-9284-9c4ecb174507.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>env.num_ip_addrs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>env.max_workers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>env.debugging</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>env.msg_level</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>env.msg_format</td>\n",
       "      <td>%(asctime)s ┇ %(message)s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>env.date_format</td>\n",
       "      <td>[%m.%d %H:%M:%S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>env.output_home</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>env.logging_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>env.argument_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>time.started</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>time.settled</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>time.elapsed</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "3a291ba614f530a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:31.923739Z",
     "start_time": "2024-04-30T11:30:31.890352Z"
    }
   },
   "source": [
    "# we will use replicate's hosted api\n",
    "import replicate\n",
    "\n",
    "\n",
    "# text completion with input prompt\n",
    "def Completion(prompt):\n",
    "    output = replicate.run(\n",
    "        model_ref,\n",
    "        input={\"prompt\": prompt,\n",
    "               \"max_new_tokens\": 1000}\n",
    "    )\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "# chat completion with input prompt and system prompt\n",
    "def ChatCompletion(prompt, system_prompt=None):\n",
    "    output = replicate.run(\n",
    "        model_ref,\n",
    "        input={\"system_prompt\": system_prompt,\n",
    "               \"prompt\": prompt,\n",
    "               \"max_new_tokens\": 1000}\n",
    "    )\n",
    "    return \"\".join(output)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "b8e638019cbe3f81",
   "metadata": {},
   "source": [
    "### **2.2 - Basic completion**"
   ]
  },
  {
   "cell_type": "code",
   "id": "20f09001024cc389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:35.652246Z",
     "start_time": "2024-04-30T11:30:31.924346Z"
    }
   },
   "source": [
    "with JobTimer(\"2.2 - Basic completion\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = Completion(prompt=\"The typical color of a llama is: \")\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:30:32] ┇ ================================================================================\n",
      "[04.30 20:30:32] ┇ [INIT] 2.2 - Basic completion\n",
      "[04.30 20:30:32] ┇ ================================================================================\n",
      "[04.30 20:30:32] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:30:33] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:34] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/2mhzrp2acsrgp0cf5v2rqt8a64 \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:35] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/2mhzrp2acsrgp0cf5v2rqt8a64 \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:35] ┇ LLM:  Hello! I'm here to help answer your question. The typical color of a llama is a safe and positive topic. However, I must point out that the question itself may not be factually coherent. Llamas do not have a \"typical\" color, as they come in a variety of colors such as white, black, gray, and brown.\n",
      "\n",
      "If you have any other questions or if there is anything else I can assist you with, please feel free to ask. I will do my best to provide a helpful and accurate response while being safe and respectful.\n",
      "[04.30 20:30:35] ┇ ================================================================================\n",
      "[04.30 20:30:35] ┇ [EXIT] 2.2 - Basic completion ($=00:00:02.890)\n",
      "[04.30 20:30:35] ┇ ================================================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "c49e96ea313fa040",
   "metadata": {},
   "source": [
    "### **2.3 - System prompts**"
   ]
  },
  {
   "cell_type": "code",
   "id": "3393c643c521904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:38.362863Z",
     "start_time": "2024-04-30T11:30:35.653250Z"
    }
   },
   "source": [
    "with JobTimer(\"2.3 - System prompts\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"The typical color of a llama is: \",\n",
    "        system_prompt=\"respond with only one word\"\n",
    "    )\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:30:35] ┇ ================================================================================\n",
      "[04.30 20:30:35] ┇ [INIT] 2.3 - System prompts\n",
      "[04.30 20:30:35] ┇ ================================================================================\n",
      "[04.30 20:30:36] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:30:37] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:37] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/jghynmar89rgm0cf5v2tgc9m1c \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:37] ┇ LLM:  Sure! Here's my response:\n",
      "\n",
      "Brown\n",
      "[04.30 20:30:38] ┇ ================================================================================\n",
      "[04.30 20:30:38] ┇ [EXIT] 2.3 - System prompts ($=00:00:01.873)\n",
      "[04.30 20:30:38] ┇ ================================================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "91dfc3b108520876",
   "metadata": {},
   "source": [
    "### **2.4 - Response formats**\n",
    "* Can support different formatted outputs e.g. text, JSON, etc."
   ]
  },
  {
   "cell_type": "code",
   "id": "2cdbd98ad3f5cda9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:41.596963Z",
     "start_time": "2024-04-30T11:30:38.365572Z"
    }
   },
   "source": [
    "with JobTimer(\"2.4 - Response formats\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"The typical color of a llama is: \",\n",
    "        system_prompt=\"response in json format\"\n",
    "    )\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:30:38] ┇ ================================================================================\n",
      "[04.30 20:30:38] ┇ [INIT] 2.4 - Response formats\n",
      "[04.30 20:30:38] ┇ ================================================================================\n",
      "[04.30 20:30:39] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:30:39] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:41] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/phxnmfk2znrgp0cf5v2vfzcer0 \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:41] ┇ LLM:  {\n",
      "\"response\": {\n",
      "\"typical_color\": \"brown\"\n",
      "}\n",
      "}\n",
      "[04.30 20:30:41] ┇ ================================================================================\n",
      "[04.30 20:30:41] ┇ [EXIT] 2.4 - Response formats ($=00:00:02.380)\n",
      "[04.30 20:30:41] ┇ ================================================================================\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3 - Gen AI Application Architecture**\n",
    "\n",
    "Here is the high-level tech stack/architecture of Generative AI application."
   ],
   "id": "9a0a627dd9fdb44c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:41.605488Z",
     "start_time": "2024-04-30T11:30:41.601174Z"
    }
   },
   "cell_type": "code",
   "source": "genai_app_arch()",
   "id": "b7175c90b89a9cac",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBmbG93Y2hhcnQgVEQKICAgICAgICBBW1VzZXJzXSAtLT4gQihBcHBsaWNhdGlvbnMgZS5nLiBtb2JpbGUsIHdlYikKICAgICAgICBCIC0tPiB8SG9zdGVkIEFQSXxDKFBsYXRmb3JtcyBlLmcuIEN1c3RvbSwgSHVnZ2luZ0ZhY2UsIFJlcGxpY2F0ZSkKICAgICAgICBCIC0tIG9wdGlvbmFsIC0tPiBFKEZyYW1ld29ya3MgZS5nLiBMYW5nQ2hhaW4pCiAgICAgICAgQy0tPnxVc2VyIElucHV0fERbTGxhbWEgMl0KICAgICAgICBELS0+fE1vZGVsIE91dHB1dHxDCiAgICAgICAgRSAtLT4gQwogICAgICAgIGNsYXNzRGVmIGRlZmF1bHQgZmlsbDojQ0NFNkZGLHN0cm9rZTojODRCQ0Y1LHRleHRDb2xvcjojMUMyQjMzLGZvbnRGYW1pbHk6dHJlYnVjaGV0IG1zOwogICAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **4 - Chatbot Architecture**\n",
    "\n",
    "Here are the key components and the information flow in a chatbot.\n",
    "* User Prompts\n",
    "* Input Safety\n",
    "* Llama 2\n",
    "* Output Safety\n",
    "* Memory & Context"
   ],
   "id": "7ca1709c6ac20994"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:41.610455Z",
     "start_time": "2024-04-30T11:30:41.606907Z"
    }
   },
   "cell_type": "code",
   "source": "bot_arch()",
   "id": "daab8e849bb8fdb2",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICB1c2VyIC0tPiBwcm9tcHQKICAgICAgICBwcm9tcHQgLS0+IGlfc2FmZXR5CiAgICAgICAgaV9zYWZldHkgLS0+IGNvbnRleHQKICAgICAgICBjb250ZXh0IC0tPiBMbGFtYV8yCiAgICAgICAgTGxhbWFfMiAtLT4gb3V0cHV0CiAgICAgICAgb3V0cHV0IC0tPiBvX3NhZmV0eQogICAgICAgIGlfc2FmZXR5IC0tPiBtZW1vcnkKICAgICAgICBvX3NhZmV0eSAtLT4gbWVtb3J5CiAgICAgICAgbWVtb3J5IC0tPiBjb250ZXh0CiAgICAgICAgb19zYWZldHkgLS0+IHVzZXIKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **4.1 - Chat conversation**\n",
    "* LLMs are stateless\n",
    "* Single Turn\n",
    "* Multi Turn (Memory)"
   ],
   "id": "7cafbf05e3b4c9c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:44.190661Z",
     "start_time": "2024-04-30T11:30:41.611556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of single turn chat\n",
    "with JobTimer(\"4.1 - Chat conversation\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"What is the average lifespan of a Llama?\",\n",
    "        system_prompt=\"answer the last question in few words\"\n",
    "    )\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "id": "e251253363dea6d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:30:41] ┇ ================================================================================\n",
      "[04.30 20:30:41] ┇ [INIT] 4.1 - Chat conversation\n",
      "[04.30 20:30:41] ┇ ================================================================================\n",
      "[04.30 20:30:42] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:30:43] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:43] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/60ypvevgssrgp0cf5v2vbnm4g4 \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:43] ┇ LLM:  Sure! The average lifespan of a llama is around 15-20 years.\n",
      "[04.30 20:30:43] ┇ ================================================================================\n",
      "[04.30 20:30:43] ┇ [EXIT] 4.1 - Chat conversation ($=00:00:01.733)\n",
      "[04.30 20:30:43] ┇ ================================================================================\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:46.441441Z",
     "start_time": "2024-04-30T11:30:44.192755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of single turn chat\n",
    "with JobTimer(\"4.1 - Chat conversation: Single Turn (1/2)\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"What is the average lifespan of a Llama?\",\n",
    "        system_prompt=\"answer the last question in few words\"\n",
    "    )\n",
    "html(f\"<div class='LLM'>{output}</div>\")"
   ],
   "id": "bdb99fc9883cb8e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:30:44] ┇ ================================================================================\n",
      "[04.30 20:30:44] ┇ [INIT] 4.1 - Chat conversation: Single Turn (1/2)\n",
      "[04.30 20:30:44] ┇ ================================================================================\n",
      "[04.30 20:30:44] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:30:45] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:46] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/adznk3kskxrgj0cf5v2stgmpmg \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:46] ┇ ================================================================================\n",
      "[04.30 20:30:46] ┇ [EXIT] 4.1 - Chat conversation: Single Turn (1/2) ($=00:00:01.395)\n",
      "[04.30 20:30:46] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div class='LLM'> Sure! The average lifespan of a llama is around 20-30 years.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:49.172045Z",
     "start_time": "2024-04-30T11:30:46.443177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example without previous context. LLM's are stateless and cannot understand \"they\" without previous context\n",
    "with JobTimer(\"4.1 - Chat conversation: Single Turn (2/2)\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"What animal family are they?\",\n",
    "        system_prompt=\"answer the last question in few words\"\n",
    "    )\n",
    "html(f\"<div class='LLM'>{output}</div>\")"
   ],
   "id": "8530c9c8b79f9eaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:30:46] ┇ ================================================================================\n",
      "[04.30 20:30:46] ┇ [INIT] 4.1 - Chat conversation: Single Turn (2/2)\n",
      "[04.30 20:30:46] ┇ ================================================================================\n",
      "[04.30 20:30:47] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:30:47] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:48] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/s3133542d9rgp0cf5v2r1hg0t8 \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:48] ┇ ================================================================================\n",
      "[04.30 20:30:48] ┇ [EXIT] 4.1 - Chat conversation: Single Turn (2/2) ($=00:00:01.887)\n",
      "[04.30 20:30:48] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div class='LLM'> Sure! Here's the answer in few words:\n",
       "\n",
       "They are from the canine family.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Chat app requires us to send in previous context to LLM to get in valid responses. Below is an example of Multi-turn chat.",
   "id": "5ed6c2e295ea021b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:30:51.446480Z",
     "start_time": "2024-04-30T11:30:49.173001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of single turn chat\n",
    "with JobTimer(\"4.1 - Chat conversation: Multi Turn (Memory)\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        system_prompt=\"answer the last question in few words\",\n",
    "        prompt=\"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: Sure! The average lifespan of a llama is around 20-30 years.\n",
    "User: What animal family are they?\n",
    "\"\"\",\n",
    "    )\n",
    "html(f\"<div class='LLM'>{output}</div>\")"
   ],
   "id": "8454cb422834ea24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 20:30:49] ┇ ================================================================================\n",
      "[04.30 20:30:49] ┇ [INIT] 4.1 - Chat conversation: Multi Turn (Memory)\n",
      "[04.30 20:30:49] ┇ ================================================================================\n",
      "[04.30 20:30:49] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 20:30:50] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:51] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/6cf9xncd3xrgg0cf5v2r761evm \"HTTP/1.1 200 OK\"\n",
      "[04.30 20:30:51] ┇ ================================================================================\n",
      "[04.30 20:30:51] ┇ [EXIT] 4.1 - Chat conversation: Multi Turn (Memory) ($=00:00:01.443)\n",
      "[04.30 20:30:51] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div class='LLM'> Sure! Llamas are members of the camelid family.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
