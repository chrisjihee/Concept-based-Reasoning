{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d680961880222a81",
   "metadata": {},
   "source": [
    "# **Getting to know Llama 2: Everything you need to start building**\n",
    "Our goal in this session is to provide a guided tour of Llama 2, including understanding different Llama 2 models, how and where to access them, Generative AI and Chatbot architectures, prompt engineering, RAG (Retrieval Augmented Generation), Fine-tuning and more. All this is implemented with a starter code for you to take it and use it in your Llama 2 projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545e17b77f35ab",
   "metadata": {},
   "source": [
    "## **0 - Prerequisites**\n",
    "* Basic understanding of Large Language Models\n",
    "* Basic understanding of Python"
   ]
  },
  {
   "cell_type": "code",
   "id": "d87dc3a5a98e8f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:08:50.536108Z",
     "start_time": "2024-04-30T10:08:49.848121Z"
    }
   },
   "source": [
    "# presentation layer code\n",
    "\n",
    "import base64\n",
    "from getpass import getpass\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "from chrisbase.data import *\n",
    "from chrisbase.io import *\n",
    "from chrisbase.util import *\n",
    "\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "\n",
    "def md(t):\n",
    "    display(Markdown(t))\n",
    "\n",
    "\n",
    "def genai_app_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[Users] --> B(Applications e.g. mobile, web)\n",
    "        B --> |Hosted API|C(Platforms e.g. Custom, HuggingFace, Replicate)\n",
    "        B -- optional --> E(Frameworks e.g. LangChain)\n",
    "        C-->|User Input|D[Llama 2]\n",
    "        D-->|Model Output|C\n",
    "        E --> C\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def rag_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[User Prompts] --> B(Frameworks e.g. LangChain)\n",
    "        B <--> |Database, Docs, XLS|C[fa:fa-database External Data]\n",
    "        B -->|API|D[Llama 2]\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "\n",
    "def llama2_family():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        llama-2 --> llama-2-7b\n",
    "        llama-2 --> llama-2-13b\n",
    "        llama-2 --> llama-2-70b\n",
    "        llama-2-7b --> llama-2-7b-chat\n",
    "        llama-2-13b --> llama-2-13b-chat\n",
    "        llama-2-70b --> llama-2-70b-chat\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def apps_and_llms():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        users --> apps\n",
    "        apps --> frameworks\n",
    "        frameworks --> platforms\n",
    "        platforms --> Llama 2\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# Create a text widget\n",
    "API_KEY = widgets.Password(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='API_KEY:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "def bot_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        user --> prompt\n",
    "        prompt --> i_safety\n",
    "        i_safety --> context\n",
    "        context --> Llama_2\n",
    "        Llama_2 --> output\n",
    "        output --> o_safety\n",
    "        i_safety --> memory\n",
    "        o_safety --> memory\n",
    "        memory --> context\n",
    "        o_safety --> user\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def fine_tuned_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        Custom_Dataset --> Pre-trained_Llama\n",
    "        Pre-trained_Llama --> Fine-tuned_Llama\n",
    "        Fine-tuned_Llama --> RLHF\n",
    "        RLHF --> |Loss:Cross-Entropy|Fine-tuned_Llama\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def load_data_faiss_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        documents --> textsplitter\n",
    "        textsplitter --> embeddings\n",
    "        embeddings --> vectorstore\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def mem_context():\n",
    "    mm(\"\"\"\n",
    "    graph LR\n",
    "        context(text)\n",
    "        user_prompt --> context\n",
    "        instruction --> context\n",
    "        examples --> context\n",
    "        memory --> context\n",
    "        context --> tokenizer\n",
    "        tokenizer --> embeddings\n",
    "        embeddings --> LLM\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "acd794f15ce1f743",
   "metadata": {},
   "source": [
    "## **1 - Understanding Llama 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548692b6a6dd66c",
   "metadata": {},
   "source": [
    "### **1.1 - What is Llama 2?**\n",
    "* State of the art (SOTA), Open Source LLM\n",
    "* 7B, 13B, 70B\n",
    "* Pretrained + Chat\n",
    "* Choosing model: Size, Quality, Cost, Speed\n",
    "* [Research paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "* [Responsible use guide](https://ai.meta.com/llama/responsible-use-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7d7c15448e3f170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:08:50.540300Z",
     "start_time": "2024-04-30T10:08:50.537378Z"
    }
   },
   "source": [
    "llama2_family()"
   ],
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTdiCiAgICAgICAgbGxhbWEtMiAtLT4gbGxhbWEtMi0xM2IKICAgICAgICBsbGFtYS0yIC0tPiBsbGFtYS0yLTcwYgogICAgICAgIGxsYW1hLTItN2IgLS0+IGxsYW1hLTItN2ItY2hhdAogICAgICAgIGxsYW1hLTItMTNiIC0tPiBsbGFtYS0yLTEzYi1jaGF0CiAgICAgICAgbGxhbWEtMi03MGIgLS0+IGxsYW1hLTItNzBiLWNoYXQKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "190b25e8cdc4c64d",
   "metadata": {},
   "source": [
    "### **1.2 - Accessing Llama 2**\n",
    "* Download + Self Host (on-premise)\n",
    "* Hosted API Platform (e.g. [Replicate](https://replicate.com/meta))\n",
    "* Hosted Container Platform (e.g. [Azure](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-llama-2-on-azure/ba-p/3881233), [AWS](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/), [GCP](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0f1cf7c500be4",
   "metadata": {},
   "source": [
    "### **1.3 - Use Cases of Llama 2**\n",
    "* Content Generation\n",
    "* Chatbots\n",
    "* Summarization\n",
    "* Programming (e.g. Code Llama)\n",
    "* and many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965614383680fb8",
   "metadata": {},
   "source": [
    "## **2 - Using Llama 2**\n",
    "In this notebook, we are going to access [Llama 13b chat model](https://replicate.com/meta/llama-2-13b-chat) using hosted API from Replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1a112c1342b73",
   "metadata": {},
   "source": [
    "### **2.1 - Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a77a8bb71e58ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:08:51.126897Z",
     "start_time": "2024-04-30T10:08:50.540954Z"
    }
   },
   "source": [
    "# Install dependencies and initialize\n",
    "!pip list | grep -E \"torch|transformers|chris|langchain|replicate|faiss|jupyter\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrisbase                 0.5.2\r\n",
      "faiss-cpu                 1.8.0\r\n",
      "jupyter                   1.0.0\r\n",
      "jupyter_client            8.6.1\r\n",
      "jupyter-console           6.6.3\r\n",
      "jupyter_core              5.7.2\r\n",
      "jupyter-events            0.10.0\r\n",
      "jupyter-lsp               2.2.5\r\n",
      "jupyter_server            2.14.0\r\n",
      "jupyter_server_terminals  0.5.3\r\n",
      "jupyterlab                4.1.8\r\n",
      "jupyterlab_pygments       0.3.0\r\n",
      "jupyterlab_server         2.27.1\r\n",
      "jupyterlab_widgets        3.0.10\r\n",
      "langchain                 0.1.16\r\n",
      "langchain-community       0.0.34\r\n",
      "langchain-core            0.1.46\r\n",
      "langchain-text-splitters  0.0.1\r\n",
      "replicate                 0.25.2\r\n",
      "sentence-transformers     2.7.0\r\n",
      "torch                     2.3.0\r\n",
      "transformers              4.40.1\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8068c18c446acaec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:08:51.131835Z",
     "start_time": "2024-04-30T10:08:51.129183Z"
    }
   },
   "source": [
    "# model url on Replicate platform that we will use for inferencing\n",
    "# We will use llama 2 13b chat model hosted on replicate server ()agent_name = \"llama-2-13b-chat\"\n",
    "model_ref = f\"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "45ec8c200c5b6f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:08:51.153291Z",
     "start_time": "2024-04-30T10:08:51.132708Z"
    }
   },
   "source": [
    "# We will use Replicate hosted cloud environment\n",
    "# Obtain Replicate API key → https://replicate.com/account/api-tokens)\n",
    "\n",
    "# enter your replicate api token\n",
    "REPLICATE_API_TOKEN = read_or(first_path_or(\".replicate*\")) or getpass()\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "\n",
    "logging.getLogger(\"IPKernelApp\").setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "args = CommonArguments(\n",
    "    env=ProjectEnv(\n",
    "        project=\"LLM-based\",\n",
    "        job_name=\"Llama 2 13b Chat\",\n",
    "        msg_level=logging.INFO,\n",
    "        msg_format=LoggingFormat.BRIEF_16,\n",
    "    )\n",
    ")\n",
    "args.dataframe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      CommonArguments                                                                                        value\n",
       "0                 tag                                                                                         None\n",
       "1         env.project                                                                                    LLM-based\n",
       "2        env.job_name                                                                             Llama 2 13b Chat\n",
       "3     env.job_version                                                                                         None\n",
       "4        env.hostname                                                                           ChrisBookPro.local\n",
       "5        env.hostaddr                                                                                  192.168.0.8\n",
       "6      env.time_stamp                                                                                  0430.190850\n",
       "7     env.python_path                                        /Users/chris/miniforge3/envs/LLM-based/bin/python3.11\n",
       "8     env.current_dir                                                                  /Users/chris/proj/LLM-based\n",
       "9    env.current_file                                    /Users/chris/proj/LLM-based/Getting_to_know_Llama_2.ipynb\n",
       "10    env.working_dir                                                                  /Users/chris/proj/LLM-based\n",
       "11   env.command_args  [-f, /Users/chris/Library/Jupyter/runtime/kernel-0faba5e0-9e07-440b-893c-a432891fd5bc.json]\n",
       "12   env.num_ip_addrs                                                                                            0\n",
       "13    env.max_workers                                                                                            1\n",
       "14      env.debugging                                                                                        False\n",
       "15      env.msg_level                                                                                           20\n",
       "16     env.msg_format                                                       %(asctime)s ┇ %(name)16s ┇ %(message)s\n",
       "17    env.date_format                                                                             [%m.%d %H:%M:%S]\n",
       "18    env.output_home                                                                                         None\n",
       "19   env.logging_file                                                                                         None\n",
       "20  env.argument_file                                                                                         None\n",
       "21       time.started                                                                                         None\n",
       "22       time.settled                                                                                         None\n",
       "23       time.elapsed                                                                                         None"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommonArguments</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tag</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>env.project</td>\n",
       "      <td>LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>env.job_name</td>\n",
       "      <td>Llama 2 13b Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>env.job_version</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>env.hostname</td>\n",
       "      <td>ChrisBookPro.local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>env.hostaddr</td>\n",
       "      <td>192.168.0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>env.time_stamp</td>\n",
       "      <td>0430.190850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>env.python_path</td>\n",
       "      <td>/Users/chris/miniforge3/envs/LLM-based/bin/python3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>env.current_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>env.current_file</td>\n",
       "      <td>/Users/chris/proj/LLM-based/Getting_to_know_Llama_2.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>env.working_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>env.command_args</td>\n",
       "      <td>[-f, /Users/chris/Library/Jupyter/runtime/kernel-0faba5e0-9e07-440b-893c-a432891fd5bc.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>env.num_ip_addrs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>env.max_workers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>env.debugging</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>env.msg_level</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>env.msg_format</td>\n",
       "      <td>%(asctime)s ┇ %(name)16s ┇ %(message)s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>env.date_format</td>\n",
       "      <td>[%m.%d %H:%M:%S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>env.output_home</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>env.logging_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>env.argument_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>time.started</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>time.settled</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>time.elapsed</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "3a291ba614f530a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:08:51.187654Z",
     "start_time": "2024-04-30T10:08:51.154078Z"
    }
   },
   "source": [
    "# we will use replicate's hosted api\n",
    "import replicate\n",
    "\n",
    "\n",
    "# text completion with input prompt\n",
    "def Completion(prompt):\n",
    "    output = replicate.run(\n",
    "        model_ref,\n",
    "        input={\"prompt\": prompt,\n",
    "               \"max_new_tokens\": 1000}\n",
    "    )\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "# chat completion with input prompt and system prompt\n",
    "def ChatCompletion(prompt, system_prompt=None):\n",
    "    output = replicate.run(\n",
    "        model_ref,\n",
    "        input={\"system_prompt\": system_prompt,\n",
    "               \"prompt\": prompt,\n",
    "               \"max_new_tokens\": 1000}\n",
    "    )\n",
    "    return \"\".join(output)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "b8e638019cbe3f81",
   "metadata": {},
   "source": [
    "### **2.2 - Basic completion**"
   ]
  },
  {
   "cell_type": "code",
   "id": "20f09001024cc389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:08:54.742220Z",
     "start_time": "2024-04-30T10:08:51.188408Z"
    }
   },
   "source": [
    "with JobTimer(\"2.2 - Basic completion\", rt=1, rb=1, rw=70, rc='=', verbose=1):\n",
    "    output = Completion(prompt=\"The typical color of a llama is: \")\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 19:08:51] ┇   chrisbase.data ┇ ======================================================================\n",
      "[04.30 19:08:51] ┇   chrisbase.data ┇ [INIT] 2.2 - Basic completion\n",
      "[04.30 19:08:51] ┇   chrisbase.data ┇ ======================================================================\n",
      "[04.30 19:08:52] ┇            httpx ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 19:08:52] ┇            httpx ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 19:08:53] ┇            httpx ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/4zv4tzw4dnrgj0cf5sx8adyk1w \"HTTP/1.1 200 OK\"\n",
      "[04.30 19:08:54] ┇            httpx ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/4zv4tzw4dnrgj0cf5sx8adyk1w \"HTTP/1.1 200 OK\"\n",
      "[04.30 19:08:54] ┇         __main__ ┇ LLM:  Hello! I'm here to help you with your question. The typical color of a llama is a safe and positive topic. However, I would like to point out that the question does not make sense, as llamas do not have a typical color. Llamas are domesticated animals that come in a variety of colors, including white, gray, brown, and black. There is no one \"typical\" color for llamas.\n",
      "\n",
      "If you have any other questions or if there is anything else I can help with, please feel free to ask. I'm here to assist you in a safe and respectful manner.\n",
      "[04.30 19:08:54] ┇   chrisbase.data ┇ ======================================================================\n",
      "[04.30 19:08:54] ┇   chrisbase.data ┇ [EXIT] 2.2 - Basic completion ($=00:00:02.712)\n",
      "[04.30 19:08:54] ┇   chrisbase.data ┇ ======================================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "c49e96ea313fa040",
   "metadata": {},
   "source": [
    "### **2.3 - System prompts**"
   ]
  },
  {
   "cell_type": "code",
   "id": "3393c643c521904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:08:56.946577Z",
     "start_time": "2024-04-30T10:08:54.743944Z"
    }
   },
   "source": [
    "with JobTimer(\"2.3 - System prompts\", rt=1, rb=1, rw=70, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"The typical color of a llama is: \",\n",
    "        system_prompt=\"respond with only one word\"\n",
    "    )\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 19:08:54] ┇   chrisbase.data ┇ ======================================================================\n",
      "[04.30 19:08:54] ┇   chrisbase.data ┇ [INIT] 2.3 - System prompts\n",
      "[04.30 19:08:54] ┇   chrisbase.data ┇ ======================================================================\n",
      "[04.30 19:08:55] ┇            httpx ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 19:08:55] ┇            httpx ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 19:08:56] ┇            httpx ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/ybcazywg29rgp0cf5sx8db6e30 \"HTTP/1.1 200 OK\"\n",
      "[04.30 19:08:56] ┇         __main__ ┇ LLM:  Sure! Here's my response:\n",
      "\n",
      "Brown\n",
      "[04.30 19:08:56] ┇   chrisbase.data ┇ ======================================================================\n",
      "[04.30 19:08:56] ┇   chrisbase.data ┇ [EXIT] 2.3 - System prompts ($=00:00:01.353)\n",
      "[04.30 19:08:56] ┇   chrisbase.data ┇ ======================================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "91dfc3b108520876",
   "metadata": {},
   "source": [
    "### **2.4 - Response formats**\n",
    "* Can support different formatted outputs e.g. text, JSON, etc."
   ]
  },
  {
   "cell_type": "code",
   "id": "2cdbd98ad3f5cda9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:08:59.148367Z",
     "start_time": "2024-04-30T10:08:56.949685Z"
    }
   },
   "source": [
    "with JobTimer(\"2.4 - Response formats\", rt=1, rb=1, rw=70, rc='=', verbose=1):\n",
    "    output = ChatCompletion(\n",
    "        prompt=\"The typical color of a llama is: \",\n",
    "        system_prompt=\"response in json format\"\n",
    "    )\n",
    "    logger.info(f\"LLM: {output}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04.30 19:08:57] ┇   chrisbase.data ┇ ======================================================================\n",
      "[04.30 19:08:57] ┇   chrisbase.data ┇ [INIT] 2.4 - Response formats\n",
      "[04.30 19:08:57] ┇   chrisbase.data ┇ ======================================================================\n",
      "[04.30 19:08:57] ┇            httpx ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[04.30 19:08:57] ┇            httpx ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[04.30 19:08:58] ┇            httpx ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/89j1gamrnnrgp0cf5sxb4v13tm \"HTTP/1.1 200 OK\"\n",
      "[04.30 19:08:58] ┇         __main__ ┇ LLM:  {\n",
      "\"response\": {\n",
      "\"typical_color\": \"white\"\n",
      "}\n",
      "}\n",
      "[04.30 19:08:58] ┇   chrisbase.data ┇ ======================================================================\n",
      "[04.30 19:08:58] ┇   chrisbase.data ┇ [EXIT] 2.4 - Response formats ($=00:00:01.357)\n",
      "[04.30 19:08:58] ┇   chrisbase.data ┇ ======================================================================\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
