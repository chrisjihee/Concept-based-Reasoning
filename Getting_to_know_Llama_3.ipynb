{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d680961880222a81",
   "metadata": {},
   "source": [
    "# **Getting to know Llama 3: Everything you need to start building**\n",
    "Our goal in this session is to provide a guided tour of Llama 3 with comparison with Llama 2, including understanding different Llama 3 models, how and where to access them, Generative AI and Chatbot architectures, prompt engineering, RAG (Retrieval Augmented Generation), Fine-tuning and more. All this is implemented with a starter code for you to take it and use it in your Llama 3 projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545e17b77f35ab",
   "metadata": {},
   "source": [
    "### **0 - Prerequisites**\n",
    "* Basic understanding of Large Language Models\n",
    "* Basic understanding of Python"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:21:56.601140Z",
     "start_time": "2024-06-18T05:21:55.343086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from chrisbase.data import *\n",
    "from chrisbase.io import *\n",
    "from chrisbase.util import *"
   ],
   "id": "3a6cabd1ff6c35a7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d87dc3a5a98e8f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:21:56.609512Z",
     "start_time": "2024-06-18T05:21:56.602940Z"
    }
   },
   "source": [
    "import base64\n",
    "from IPython.display import display, Image, Markdown, HTML\n",
    "\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "\n",
    "def md(t):\n",
    "    display(Markdown(t))\n",
    "\n",
    "\n",
    "def hr():\n",
    "    display(HTML(\"<hr style='border:3px solid MidnightBlue; width:880px; padding:0;\"\n",
    "                 \" margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>\"))\n",
    "\n",
    "\n",
    "def mds(ps: dict):\n",
    "    for k, v in ps.items():\n",
    "        hr()\n",
    "        md(f\"**[{k}]**\\n{prefixed_str(v, '> ')}\")\n",
    "    hr()\n",
    "\n",
    "\n",
    "def html(t,\n",
    "         s=\"<pre style='font-family:Arial; font-size:12pt;\"\n",
    "           \" padding:10px; margin-left:10px; margin-top:10px;\"\n",
    "           \" background-color:LightSkyBlue; border:3px solid MidnightBlue;\"\n",
    "           \" width:100%; height:100px;'>\",\n",
    "         e=\"</pre>\"):\n",
    "    display(HTML(s + t + e))\n",
    "\n",
    "\n",
    "def llama3_family():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        llama-3 --> llama-3-8b\n",
    "        llama-3 --> llama-3-70b\n",
    "        llama-3-8b --> llama-3-8b\n",
    "        llama-3-8b --> llama-3-8b-instruct\n",
    "        llama-3-70b --> llama-3-70b\n",
    "        llama-3-70b --> llama-3-70b-instruct\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def genai_app_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[Users] --> B(Applications e.g. mobile, web)\n",
    "        B --> |Hosted API|C(Platforms e.g. Custom, HuggingFace, Replicate)\n",
    "        B -- optional --> E(Frameworks e.g. LangChain)\n",
    "        C-->|User Input|D[Llama 3]\n",
    "        D-->|Model Output|C\n",
    "        E --> C\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def bot_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        user --> prompt\n",
    "        prompt --> i_safety\n",
    "        i_safety --> context\n",
    "        context --> Llama_3\n",
    "        Llama_3 --> output\n",
    "        output --> o_safety\n",
    "        i_safety --> memory\n",
    "        o_safety --> memory\n",
    "        memory --> context\n",
    "        o_safety --> user\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def mem_context():\n",
    "    mm(\"\"\"\n",
    "    graph LR\n",
    "        context(text)\n",
    "        user_prompt --> context\n",
    "        instruction --> context\n",
    "        examples --> context\n",
    "        memory --> context\n",
    "        context --> tokenizer\n",
    "        tokenizer --> embeddings\n",
    "        embeddings --> LLM\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def rag_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[User Prompts] --> B(Frameworks e.g. LangChain)\n",
    "        B <--> |Database, Docs, XLS|C[fa:fa-database External Data]\n",
    "        B -->|API|D[Llama 3]\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def load_data_faiss_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        documents --> textsplitter\n",
    "        textsplitter --> embeddings\n",
    "        embeddings --> vectorstore\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def fine_tuned_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        Custom_Dataset --> Pre-trained_Llama\n",
    "        Pre-trained_Llama --> Fine-tuned_Llama\n",
    "        Fine-tuned_Llama --> RLHF\n",
    "        RLHF --> |Loss:Cross-Entropy|Fine-tuned_Llama\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "acd794f15ce1f743",
   "metadata": {},
   "source": "## **1 - Understanding Llama 3**"
  },
  {
   "cell_type": "markdown",
   "id": "548692b6a6dd66c",
   "metadata": {},
   "source": [
    "### **1.1 - What is Llama 3?**\n",
    "\n",
    "* State of the art (SOTA), Open Source LLM\n",
    "* 8B, 70B - base and instruct models\n",
    "* Choosing model: Size, Quality, Cost, Speed\n",
    "* Pretrained + Chat\n",
    "* [Meta Llama 3 Blog](https://ai.meta.com/blog/meta-llama-3/)\n",
    "* [Getting Started with Meta Llama](https://llama.meta.com/docs/get-started)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:21:56.730706Z",
     "start_time": "2024-06-18T05:21:56.610452Z"
    }
   },
   "cell_type": "code",
   "source": "llama3_family()",
   "id": "a2ef68ece727fa3f",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBsbGFtYS0zIC0tPiBsbGFtYS0zLThiCiAgICAgICAgbGxhbWEtMyAtLT4gbGxhbWEtMy03MGIKICAgICAgICBsbGFtYS0zLThiIC0tPiBsbGFtYS0zLThiCiAgICAgICAgbGxhbWEtMy04YiAtLT4gbGxhbWEtMy04Yi1pbnN0cnVjdAogICAgICAgIGxsYW1hLTMtNzBiIC0tPiBsbGFtYS0zLTcwYgogICAgICAgIGxsYW1hLTMtNzBiIC0tPiBsbGFtYS0zLTcwYi1pbnN0cnVjdAogICAgICAgIGNsYXNzRGVmIGRlZmF1bHQgZmlsbDojQ0NFNkZGLHN0cm9rZTojODRCQ0Y1LHRleHRDb2xvcjojMUMyQjMzLGZvbnRGYW1pbHk6dHJlYnVjaGV0IG1zOwogICAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "190b25e8cdc4c64d",
   "metadata": {},
   "source": [
    "### **1.2 - Accessing Llama 3**\n",
    "* Download + Self Host (i.e. [download Llama](https://ai.meta.com/resources/models-and-libraries/llama-downloads))\n",
    "* Hosted API Platform (e.g. [Groq](https://console.groq.com/), [Replicate](https://replicate.com/meta/meta-llama-3-8b-instruct), [Together](https://api.together.xyz/playground/language/meta-llama/Llama-3-8b-hf), [Anyscale](https://app.endpoints.anyscale.com/playground))\n",
    "* Hosted Container Platform (e.g. [Azure](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-llama-2-on-azure/ba-p/3881233), [AWS](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/), [GCP](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0f1cf7c500be4",
   "metadata": {},
   "source": [
    "### **1.3 - Use Cases of Llama 3**\n",
    "* Content Generation\n",
    "* Summarization\n",
    "* General Chatbots\n",
    "* RAG (Retrieval Augmented Generation): Chat about Your Own Data\n",
    "* Fine-tuning\n",
    "* Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965614383680fb8",
   "metadata": {},
   "source": [
    "## **2 - Using and Comparing Llama 3 and Llama 2**\n",
    "\n",
    "We will be using Llama 2 7b & 70b chat and Llama 3 8b & 70b instruct models hosted on [Replicate](https://replicate.com/search?query=llama) to run the examples here. \n",
    "You will need to first sign in with Replicate with your github account, then create a free API token [here](https://replicate.com/account/api-tokens) that you can use for a while. \n",
    "You can also use other Llama 3 cloud providers such as [Groq](https://console.groq.com/), [Together](https://api.together.xyz/playground/language/meta-llama/Llama-3-8b-hf), or [Anyscale](https://app.endpoints.anyscale.com/playground)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1a112c1342b73",
   "metadata": {},
   "source": [
    "### **2.1 - Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a77a8bb71e58ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:21:58.420544Z",
     "start_time": "2024-06-18T05:21:56.734145Z"
    }
   },
   "source": [
    "# Install dependencies and initialize\n",
    "!pip list | grep -E \"langchain|transformers|torch|faiss|groq|replicate|bs4|chris\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs4                       0.0.2\r\n",
      "chrisbase                 0.5.3\r\n",
      "faiss-cpu                 1.8.0\r\n",
      "groq                      0.5.0\r\n",
      "langchain                 0.1.20\r\n",
      "langchain-community       0.0.38\r\n",
      "langchain-core            0.1.52\r\n",
      "langchain-groq            0.1.5\r\n",
      "langchain-text-splitters  0.0.1\r\n",
      "replicate                 0.25.2\r\n",
      "sentence-transformers     2.7.0\r\n",
      "torch                     2.3.0\r\n",
      "transformers              4.40.2\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.2 - Create helpers for Llama 2 and Llama 3**\n",
    "First, set your API token as environment variables."
   ],
   "id": "da1eb02f1aa9b632"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:21:58.459230Z",
     "start_time": "2024-06-18T05:21:58.423931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from getpass import getpass\n",
    "\n",
    "GROQ_API_TOKEN = read_or(first_path_or(\"*groq*.key\")) or getpass()\n",
    "REPLICATE_API_TOKEN = read_or(first_path_or(\"*replicate*.key\")) or getpass()\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_TOKEN\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "print(f\"Groq API Key: {mask_str(GROQ_API_TOKEN, start=4, end=-4)}\")\n",
    "print(f\"Replicate API Key: {mask_str(REPLICATE_API_TOKEN, start=3, end=-3)}\")\n",
    "\n",
    "logging.getLogger(\"IPKernelApp\").setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "args = CommonArguments(\n",
    "    env=ProjectEnv(\n",
    "        project=\"LLM-based\",\n",
    "        job_name=\"LLaMA-based-Chat\",\n",
    "        msg_level=logging.INFO,\n",
    "        msg_format=LoggingFormat.PRINT_00,\n",
    "    )\n",
    ")\n",
    "args.dataframe()"
   ],
   "id": "3f33f0fc379c4f04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API Key: gsk_************************************************2Uuw\n",
      "Replicate API Key: r8_**********************************MYA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      CommonArguments                                                                                            value\n",
       "0                 tag                                                                                             None\n",
       "1         env.project                                                                                        LLM-based\n",
       "2        env.job_name                                                                                 LLaMA-based-Chat\n",
       "3     env.job_version                                                                                             None\n",
       "4        env.hostname                                                                                         chris137\n",
       "5        env.hostaddr                                                                                  129.254.164.137\n",
       "6      env.time_stamp                                                                                      0618.142156\n",
       "7     env.python_path                                             /home/chris/miniforge3/envs/LLM-based/bin/python3.11\n",
       "8     env.current_dir                                                                              /dat/proj/LLM-based\n",
       "9    env.current_file                                         /home/chris/proj/LLM-based/Getting_to_know_Llama_3.ipynb\n",
       "10    env.working_dir                                                                              /dat/proj/LLM-based\n",
       "11   env.command_args  [-f, /home/chris/.local/share/jupyter/runtime/kernel-dc30ee4b-a2c8-4301-a605-446a0971db99.json]\n",
       "12   env.num_ip_addrs                                                                                                1\n",
       "13    env.max_workers                                                                                                1\n",
       "14      env.debugging                                                                                            False\n",
       "15      env.msg_level                                                                                               20\n",
       "16     env.msg_format                                                                                      %(message)s\n",
       "17    env.date_format                                                                                 [%m.%d %H:%M:%S]\n",
       "18    env.output_home                                                                                             None\n",
       "19   env.logging_file                                                                                             None\n",
       "20  env.argument_file                                                                                             None\n",
       "21       time.started                                                                                             None\n",
       "22       time.settled                                                                                             None\n",
       "23       time.elapsed                                                                                             None"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommonArguments</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tag</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>env.project</td>\n",
       "      <td>LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>env.job_name</td>\n",
       "      <td>LLaMA-based-Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>env.job_version</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>env.hostname</td>\n",
       "      <td>chris137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>env.hostaddr</td>\n",
       "      <td>129.254.164.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>env.time_stamp</td>\n",
       "      <td>0618.142156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>env.python_path</td>\n",
       "      <td>/home/chris/miniforge3/envs/LLM-based/bin/python3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>env.current_dir</td>\n",
       "      <td>/dat/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>env.current_file</td>\n",
       "      <td>/home/chris/proj/LLM-based/Getting_to_know_Llama_3.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>env.working_dir</td>\n",
       "      <td>/dat/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>env.command_args</td>\n",
       "      <td>[-f, /home/chris/.local/share/jupyter/runtime/kernel-dc30ee4b-a2c8-4301-a605-446a0971db99.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>env.num_ip_addrs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>env.max_workers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>env.debugging</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>env.msg_level</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>env.msg_format</td>\n",
       "      <td>%(message)s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>env.date_format</td>\n",
       "      <td>[%m.%d %H:%M:%S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>env.output_home</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>env.logging_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>env.argument_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>time.started</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>time.settled</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>time.elapsed</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create Llama 2 and Llama 3 helper functions - for chatbot type of apps, we'll use Llama 3 instruct and Llama 2 chat models, not the base models.",
   "id": "e60f5e428045f58b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:21:58.587491Z",
     "start_time": "2024-06-18T05:21:58.460833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import replicate\n",
    "\n",
    "\n",
    "def llama2_7b(prompt):\n",
    "    output = replicate.run(\n",
    "        \"meta/llama-2-7b-chat\",\n",
    "        input={\"prompt\": prompt}\n",
    "    )\n",
    "    return ''.join(output)\n",
    "\n",
    "\n",
    "def llama2_70b(prompt):\n",
    "    output = replicate.run(\n",
    "        \"meta/llama-2-70b-chat\",\n",
    "        input={\"prompt\": prompt}\n",
    "    )\n",
    "    return ''.join(output)\n",
    "\n",
    "\n",
    "def llama3_8b(prompt):\n",
    "    output = replicate.run(\n",
    "        \"meta/meta-llama-3-8b-instruct\",\n",
    "        input={\"prompt\": prompt}\n",
    "    )\n",
    "    return ''.join(output)\n",
    "\n",
    "\n",
    "def llama3_70b(prompt):\n",
    "    output = replicate.run(\n",
    "        \"meta/meta-llama-3-70b-instruct\",\n",
    "        input={\"prompt\": prompt}\n",
    "    )\n",
    "    return ''.join(output)"
   ],
   "id": "dff82441129a04d1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:21:58.714143Z",
     "start_time": "2024-06-18T05:21:58.589050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "def llama3_8b_g(prompt, temperature=0.0, input_print=True):\n",
    "    model_ref = \"llama3-8b-8192\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model_ref,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def llama3_70b_g(prompt, temperature=0.0, input_print=True):\n",
    "    model_ref = \"llama3-70b-8192\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model_ref,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ],
   "id": "bb7eba6f7af15ebc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.3 - Basic QA with Llama 2 and 3**",
   "id": "6b3e0c0e9cf2a4a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:02.785046Z",
     "start_time": "2024-06-18T05:21:58.715463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"The typical color of a llama is: \"\n",
    "outputs = dict()\n",
    "with JobTimer(\"2.3 - Basic QA with Llama 2 and 3 (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama2_7b\"] = llama2_7b(prompt)\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "f7f3fe011bb650",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 2.3 - Basic QA with Llama 2 and 3 (1)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/5w0x3qkvp9rgm0cg575sp10r2w \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/3waw0wbzmhrgj0cg575s9ndj9g \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/3waw0wbzmhrgj0cg575s9ndj9g \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 2.3 - Basic QA with Llama 2 and 3 (1) ($=00:00:03.234)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b]**\n>  Great! I'm here to help. The typical color of a llama is actually a range of colors, including:\n> \n> * White\n> * Cream\n> * Beige\n> * Grey\n> * Brown\n> * Black\n> \n> Llamas can also have patches of different colors on their coats, such as white patches on their faces or legs. So, while there is no one \"typical\" color of a llama, they can come in a variety of colors and patterns. Is there anything else I can help you with?"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> The typical color of a llama is white, but they can also come in a variety of other colors and patterns, such as:\n> \n> * Light brown\n> * Dark brown\n> * Gray\n> * Black\n> * Fawn\n> * Cream\n> * Silver\n> * Gold\n> * Bay\n> * Red\n> * Roan (a mix of white and another color)\n> \n> Some llamas may also have patches or markings of different colors on their faces, legs, or bodies."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:06.302598Z",
     "start_time": "2024-06-18T05:22:02.788185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"The typical color of a llama is what? Answer in one word.\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"2.3 - Basic QA with Llama 2 and 3 (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama2_7b\"] = llama2_7b(prompt)\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "ee8dac91906817bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 2.3 - Basic QA with Llama 2 and 3 (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/1ce5qfwb1xrgg0cg575rhj6jcw \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/6n4a23wgasrgj0cg575t5yyxqr \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 2.3 - Basic QA with Llama 2 and 3 (2) ($=00:00:02.687)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b]**\n>  Great! I'm happy to help. The typical color of a llama is gray."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> White."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **3 - Chat conversation**",
   "id": "7cafbf05e3b4c9c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **3.1 - Single-turn chat**",
   "id": "f712d948378b5c14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:11.766127Z",
     "start_time": "2024-06-18T05:22:06.307836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example without previous context. LLM's are stateless and cannot understand \"they\" without previous context\n",
    "prompt = \"What animal family are they? Answer the question in few words.\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.1 - Single-turn chat\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama2_7b\"] = llama2_7b(prompt)\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "    outputs[\"llama2_70b\"] = llama2_70b(prompt)\n",
    "    outputs[\"llama3_70b\"] = llama3_70b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "554690216a856f7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.1 - Single-turn chat\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/30p43awqh5rgp0cg575skc2w68 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/0701y14vdnrgj0cg575r2k2xpg \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/nnd5wymza5rgg0cg575rnye23g \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/0c46gtx50nrj00cg575stzkrg4 \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.1 - Single-turn chat ($=00:00:04.622)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b]**\n>  Sure, I'd be happy to help! The animal family that the question is referring to is... (pausing for dramatic effect) ...the Canidae! 🐕🐕🐩"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> Canine family."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b]**\n>  They are primates."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b]**\n> \n> \n> Please provide the name of the animal you'd like to know the family of."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 3 70b doesn't hallucinate.**",
   "id": "98e4863c5dd3a587"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.2 - Multi-turn chat**\n",
    "Chat app requires us to send in previous context to LLM to get in valid responses. Below is an example of Multi-turn chat."
   ],
   "id": "cd8fb3eef8d283a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:14.538271Z",
     "start_time": "2024-06-18T05:22:11.768981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of multi-turn chat, with storing previous context\n",
    "prompt = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: 15-20 years.\n",
    "User: What animal family are they?\n",
    "\"\"\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.2 - Multi-turn chat (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama2_7b\"] = llama2_7b(prompt)\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "dc83a7bc22016551",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.2 - Multi-turn chat (1)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/676exqncsdrgg0cg575tp6fa10 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/t7q3aqngksrgm0cg575rmvf07m \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.2 - Multi-turn chat (1) ($=00:00:01.948)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b]**\n>  Assistant: Thank you for your question! Llamas are actually members of the camel family, not an animal family. They are domesticated mammals that are known for their distinctive long necks, ears, and coats. The average lifespan of a llama is around 15-20 years, depending on their breed, living conditions, and health. Is there anything else I can help you with?"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> The llama belongs to the camelid family, which also includes camels and alpacas."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 2 and 3 both behave well for using the chat history for follow up questions.**",
   "id": "fee4dc80f4437c2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:18.621066Z",
     "start_time": "2024-06-18T05:22:14.539318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of multi-turn chat, with storing previous context\n",
    "prompt = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: Sure! The average lifespan of a llama is around 20-30 years.\n",
    "User: What animal family are they?\n",
    "\n",
    "Answer the question with one word.\n",
    "\"\"\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.2 - Multi-turn chat (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama2_7b\"] = llama2_7b(prompt)\n",
    "    outputs[\"llama2_70b\"] = llama2_70b(prompt)\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "eaa41920ac7411b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.2 - Multi-turn chat (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/j3r29cdrynrgg0cg575rfr9ec8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-70b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/vgekjz5wqnrgm0cg575srsefhg \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/cvcpa0y0g5rgj0cg575vbvx3j8 \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.2 - Multi-turn chat (2) ($=00:00:03.250)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b]**\n>  Sure, I'd be happy to help! The animal family that llamas belong to is called \"Cervidae.\""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_70b]**\n>  Camelids"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> Camelidae"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Both Llama 3 8b and Llama 2 70b follows instructions (e.g. \"Answer the question with one word\") better than Llama 2 7b in multi-turn chat.**",
   "id": "73a04476c5892b97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.3 - Prompt Engineering**\n",
    "* Prompt engineering refers to the science of designing effective prompts to get desired responses\n",
    "* Helps reduce hallucination"
   ],
   "id": "5ae6ff4bba0e4d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **3.3.1 - In-Context Learning (e.g. Zero-shot, Few-shot)**\n",
    " * In-context learning - specific method of prompt engineering where demonstration of task are provided as part of prompt.\n",
    "  1. Zero-shot learning - model is performing tasks without any\n",
    "input examples.\n",
    "  2. Few or “N-Shot” Learning - model is performing and behaving based on input examples in user's prompt."
   ],
   "id": "195ccff05618fb37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:21.774189Z",
     "start_time": "2024-06-18T05:22:18.623335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Zero-shot example. To get positive/negative/neutral sentiment, we need to give examples in the prompt\n",
    "prompt = '''\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment: ?\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.1 - In-Context Learning (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama2_7b\"] = llama2_7b(prompt)\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "29b2d6ac41db8de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.1 - In-Context Learning (1)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/b1m57fy7m5rgm0cg575v1m118r \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/5p05gmyctxrgg0cg575t0zz63r \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.1 - In-Context Learning (1) ($=00:00:02.318)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b]**\n>  Interesting! 😊"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> Neutral"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 3 has different opinions than Llama 2.**",
   "id": "4f32d037ab6fc4b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:24.574094Z",
     "start_time": "2024-06-18T05:22:21.776783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By giving examples to Llama, it understands the expected output format.\n",
    "prompt = '''\n",
    "Classify: I love Llamas!\n",
    "Sentiment: Positive\n",
    "Classify: I dont like Snakes.\n",
    "Sentiment: Negative\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment:\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.1 - In-Context Learning (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama2_7b\"] = llama2_7b(prompt)\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "82e85255a9c01c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.1 - In-Context Learning (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/llama-2-7b-chat/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/x69k92yky1rgg0cg575v20bshr \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/y8x9t2yqrsrgm0cg575sr1n95g \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.1 - In-Context Learning (2) ($=00:00:01.968)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama2_7b]**\n>  Sure, I'd be happy to help! Here are the classifications and sentiments for the given statements:\n> \n> Classify: I love Llamas!\n> Sentiment: Positive\n> \n> Classify: I dont like Snakes.\n> Sentiment: Negative\n> \n> Classify: I saw a Gecko.\n> Sentiment: Neutral"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> Neutral"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 2, with few shots, has the same output \"Neutral\" as Llama 3, but Llama 2 doesn't follow instructions (Give one word response) well.**",
   "id": "9b26bbe1b025b17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **3.3.2 - Chain of Thought**\n",
    "\"Chain of thought\" enables complex reasoning through logical step by step thinking and generates meaningful and contextually relevant responses."
   ],
   "id": "ad3dfa53d543dbe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:27.909944Z",
     "start_time": "2024-06-18T05:22:24.576245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standard prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does Llama have?\n",
    "\n",
    "Answer in one word.\n",
    "'''\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.2 - Chain of Thought (1)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "    outputs[\"llama3_70b\"] = llama3_70b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "58aac39e8c3e5920",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.2 - Chain of Thought (1)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/yngng3pyvdrgm0cg575v3s56a0 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/3y6nayq48xrj20cg575twjaj1w \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.2 - Chain of Thought (1) ($=00:00:02.507)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> Eight."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b]**\n> \n> \n> Eleven."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 3-8b did not get the right answer because it was asked to answer in one word.**",
   "id": "84a3d62c402a3422"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:34.452996Z",
     "start_time": "2024-06-18T05:22:27.911893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By default, Llama 3 models follow \"Chain-Of-Thought\" prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does Llama have?\n",
    "'''\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.2 - Chain of Thought (2)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "    outputs[\"llama3_70b\"] = llama3_70b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "16d7e505e95b773e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.2 - Chain of Thought (2)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/0n275zfd7drgj0cg575rpn9wmm \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/0n275zfd7drgj0cg575rpn9wmm \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/32vts67mcnrj20cg575vnw1pkr \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/32vts67mcnrj20cg575vnw1pkr \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/32vts67mcnrj20cg575vnw1pkr \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/32vts67mcnrj20cg575vnw1pkr \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.2 - Chain of Thought (2) ($=00:00:05.714)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> I'd be happy to help!\n> \n> Llama started with 5 tennis balls. Then, it buys 2 more cans of tennis balls, and each can has 3 tennis balls. So, Llama buys a total of 2 x 3 = 6 tennis balls.\n> \n> Adding these 6 new tennis balls to the 5 Llama already had, we get:\n> \n> 5 + 6 = 11\n> \n> So, Llama now has a total of 11 tennis balls!"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b]**\n> \n> \n> Let's help Llama count its tennis balls!\n> \n> Llama started with 5 tennis balls.\n> \n> It buys 2 more cans of tennis balls, and each can has 3 tennis balls. So, it gets 2 x 3 = 6 new tennis balls.\n> \n> Now, let's add the new tennis balls to the ones Llama already had: 5 (initial tennis balls) + 6 (new tennis balls) = 11\n> \n> Llama now has 11 tennis balls!"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: By default, Llama 3 models identify word problems and solves it step by step!**",
   "id": "4766b54fdc66fc97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:43.094564Z",
     "start_time": "2024-06-18T05:22:34.455231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "outputs = dict()\n",
    "with JobTimer(\"3.3.2 - Chain of Thought (3)\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    outputs[\"llama3_8b\"] = llama3_8b(prompt)\n",
    "    outputs[\"llama3_70b\"] = llama3_70b(prompt)\n",
    "mds(outputs)"
   ],
   "id": "8e775aa94222cd18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.3.2 - Chain of Thought (3)\n",
      "==================================================================================================================\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/c1ahagr5dnrgj0cg57695sn2r8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/c1ahagr5dnrgj0cg57695sn2r8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/c1ahagr5dnrgj0cg57695sn2r8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/c1ahagr5dnrgj0cg57695sn2r8 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.replicate.com/v1/models/meta/meta-llama-3-70b-instruct/predictions \"HTTP/1.1 201 Created\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/ve1wbsrhxsrj20cg576be5xj9m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/ve1wbsrhxsrj20cg576be5xj9m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/ve1wbsrhxsrj20cg576be5xj9m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/ve1wbsrhxsrj20cg576be5xj9m \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.replicate.com/v1/predictions/ve1wbsrhxsrj20cg576be5xj9m \"HTTP/1.1 200 OK\"\n",
      "==================================================================================================================\n",
      "[EXIT] 3.3.2 - Chain of Thought (3) ($=00:00:07.812)\n",
      "==================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_8b]**\n> \n> \n> Yes.\n> \n> Here's the step-by-step breakdown:\n> \n> 1. We have 15 people who want to go to the restaurant.\n> 2. We have 2 cars that can seat 5 people each. This means we can fit a total of 10 people in the cars (2 x 5 = 10).\n> 3. We have 2 people who have motorcycles that can fit 2 people each. This means we can fit a total of 4 people on the motorcycles (2 x 2 = 4).\n> 4. We still have 15 - 10 - 4 = 1 person left who cannot fit in the cars or motorcycles.\n> 5. Since we have only 1 person left who cannot fit, and we have 2 cars and 2 motorcycles, we can fit all 15 people in the cars and motorcycles.\n> \n> Therefore, yes, we can all get to the restaurant by car or motorcycle."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**[llama3_70b]**\n> \n> \n> **Answer:** NO\n> \n> Here's the step-by-step explanation:\n> \n> 1. Let's start with the cars: We have 2 cars, each seating 5 people. That means we can transport a total of 2 x 5 = 10 people by car.\n> 2. We still have 15 - 10 = 5 people left to transport.\n> 3. Now, let's look at the motorcycles: We have 2 motorcycles, each fitting 2 people. That means we can transport a total of 2 x 2 = 4 people by motorcycle.\n> 4. We still have 5 - 4 = 1 person left to transport, but we don't have any more vehicles to accommodate them.\n> 5. Since we can't transport all 15 people using the available cars and motorcycles, the answer is NO."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin-left:0; margin-right:0; margin-top:10px; margin-bottom:15px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: Llama 3 70b model works correctly in this example.**",
   "id": "2a65a61a6e158af4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Summary: Llama 2 often needs encouragement for step by step thinking to correctly reasoning. Llama 3 understands, reasons and explains better, making chain of thought unnecessary in the cases above.**",
   "id": "6934a03096c51964"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.4 - Retrieval Augmented Generation (RAG)**\n",
    "* Prompt Eng Limitations - Knowledge cutoff & lack of specialized data\n",
    "* Retrieval Augmented Generation(RAG) allows us to retrieve snippets of information from external data sources and augment it to the user's prompt to get tailored responses from Llama 2.\n",
    "\n",
    "For our demo, we are going to download an external PDF file from a URL and query against the content in the pdf file to get contextually relevant information back with the help of Llama!"
   ],
   "id": "1905410ca49b6e36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:43.103844Z",
     "start_time": "2024-06-18T05:22:43.097378Z"
    }
   },
   "cell_type": "code",
   "source": "rag_arch()",
   "id": "17bd3c2b24aa01cc",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBmbG93Y2hhcnQgVEQKICAgICAgICBBW1VzZXIgUHJvbXB0c10gLS0+IEIoRnJhbWV3b3JrcyBlLmcuIExhbmdDaGFpbikKICAgICAgICBCIDwtLT4gfERhdGFiYXNlLCBEb2NzLCBYTFN8Q1tmYTpmYS1kYXRhYmFzZSBFeHRlcm5hbCBEYXRhXQogICAgICAgIEIgLS0+fEFQSXxEW0xsYW1hIDNdCiAgICAgICAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAgICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **3.4.1 - LangChain**\n",
    "LangChain is a framework that helps make it easier to implement RAG."
   ],
   "id": "e41075a070214735"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **3.4.2 - LangChain Q&A Retriever**\n",
    "* ConversationalRetrievalChain\n",
    "* Query the Source documents"
   ],
   "id": "aeb97297c614ef7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:43.165195Z",
     "start_time": "2024-06-18T05:22:43.106324Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_faiss_arch()",
   "id": "99da4d3309a10773",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBkb2N1bWVudHMgLS0+IHRleHRzcGxpdHRlcgogICAgICAgIHRleHRzcGxpdHRlciAtLT4gZW1iZWRkaW5ncwogICAgICAgIGVtYmVkZGluZ3MgLS0+IHZlY3RvcnN0b3JlCiAgICAgICAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAgICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:43.770317Z",
     "start_time": "2024-06-18T05:22:43.168231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import OnlinePDFLoader, WebBaseLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import Replicate\n",
    "import bs4"
   ],
   "id": "d2e9968dab11f08c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:22:55.474269Z",
     "start_time": "2024-06-18T05:22:43.771998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"3.4.2 - LangChain [1/4]\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    # Step 1: Load the document from a web url\n",
    "    loader = WebBaseLoader([\"https://huggingface.co/blog/llama3\"])\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Step 2: Split the document into chunks with a specified chunk size\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Step 3: Store the document into a vector store with a specific embedding model\n",
    "    vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ],
   "id": "442bef330ddd6394",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================\n",
      "[INIT] 3.4.2 - LangChain [1/4]\n",
      "==================================================================================================================\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniforge3/envs/LLM-based/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/LLM-based/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use pytorch device_name: cuda\n",
      "Loading faiss with AVX512 support.\n",
      "Successfully loaded faiss with AVX512 support.\n",
      "==================================================================================================================\n",
      "[EXIT] 3.4.2 - LangChain [1/4] ($=00:00:10.882)\n",
      "==================================================================================================================\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use the Llama 2 model hosted on Replicate\n",
    "# Temperature: Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic, 0.75 is a good starting value\n",
    "# top_p: When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens\n",
    "# max_new_tokens: Maximum number of tokens to generate. A word is generally 2-3 tokens\n",
    "llm_ref = Replicate(\n",
    "    model=model_ref,\n",
    "    model_kwargs={\"temperature\": 0.75, \"top_p\": 1, \"max_new_tokens\": 1000}\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm_ref, document_vector.as_retriever(), return_source_documents=True)\n",
    "md(f\"{type(chain)}\")"
   ],
   "id": "bfad7ef283df7b39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Query against your own data\n",
    "chat_history = []\n",
    "query = \"How is Meta approaching open science in two short sentences?\"\n",
    "with JobTimer(\"4.3.2 - LangChain Q&A Retriever [1/2]\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "html(result['answer'])"
   ],
   "id": "8b8a69f63c4b1b08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This time your previous question and answer will be included as a chat history which will enable the ability\n",
    "# to ask follow up questions.\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"How is it benefiting the world?\"\n",
    "with JobTimer(\"4.3.2 - LangChain Q&A Retriever [2/2]\", rt=1, rb=1, rw=114, rc='=', verbose=1):\n",
    "    result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "html(result['answer'])"
   ],
   "id": "d7741a0aa701af69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **5 - Fine-Tuning Models**\n",
    "* Limitatons of Prompt Eng and RAG\n",
    "* Fine-Tuning Arch\n",
    "* Types (PEFT, LoRA, QLoRA)\n",
    "* Using PyTorch for Pre-Training & Fine-Tuning\n",
    "* Evals + Quality"
   ],
   "id": "485ed0b25f0baab6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fine_tuned_arch()",
   "id": "44df90baa5f9d992",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6 - Responsible AI**\n",
    "* Power + Responsibility\n",
    "* Hallucinations\n",
    "* Input & Output Safety\n",
    "* Red-teaming (simulating real-world cyber attackers)\n",
    "* [Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/)"
   ],
   "id": "20d9ee189c8ef88d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **7 - Conclusion**\n",
    "* Active research on LLMs and Llama\n",
    "* Leverage the power of Llama and its open community\n",
    "* Safety and responsible use is paramount!\n",
    "* Call-To-Action\n",
    "  * [Replicate Free Credits](https://replicate.fyi/connect2023) for Connect attendees!\n",
    "  * This notebook is available through Llama Github recipes\n",
    "  * Use Llama in your projects and give us feedback"
   ],
   "id": "8e65302a917a841b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Resources**\n",
    "- [GitHub - Llama 2](https://github.com/facebookresearch/llama)\n",
    "- [Github - LLama 2 Recipes](https://github.com/facebookresearch/llama-recipes)\n",
    "- [Llama 2](https://ai.meta.com/llama/)\n",
    "- [Research Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "- [Model Card](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)\n",
    "- [Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/)\n",
    "- [Acceptable Use Policy](https://ai.meta.com/llama/use-policy/)\n",
    "- [Replicate](https://replicate.com/meta/)\n",
    "- [LangChain](https://www.langchain.com/)"
   ],
   "id": "d201a9aa70cfcad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Authors & Contact**\n",
    "  * asangani@meta.com, [Amit Sangani | LinkedIn](https://www.linkedin.com/in/amitsangani/)\n",
    "  * mohsena@meta.com, [Mohsen Agsen | LinkedIn](https://www.linkedin.com/in/mohsen-agsen-62a9791/)\n"
   ],
   "id": "bf0012709004082a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
