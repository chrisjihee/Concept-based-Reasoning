{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d680961880222a81",
   "metadata": {},
   "source": [
    "# **Getting to know Llama 3: Everything you need to start building**\n",
    "Our goal in this session is to provide a guided tour of Llama 3 with comparison with Llama 2, including understanding different Llama 3 models, how and where to access them, Generative AI and Chatbot architectures, prompt engineering, RAG (Retrieval Augmented Generation), Fine-tuning and more. All this is implemented with a starter code for you to take it and use it in your Llama 3 projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545e17b77f35ab",
   "metadata": {},
   "source": [
    "### **0 - Prerequisites**\n",
    "* Basic understanding of Large Language Models\n",
    "* Basic understanding of Python"
   ]
  },
  {
   "cell_type": "code",
   "id": "d87dc3a5a98e8f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:15.618577Z",
     "start_time": "2024-05-02T08:47:15.613779Z"
    }
   },
   "source": [
    "import base64\n",
    "from getpass import getpass\n",
    "\n",
    "import replicate\n",
    "from IPython.display import display, Image, Markdown, HTML\n",
    "from groq import Groq\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import Replicate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from chrisbase.data import *\n",
    "from chrisbase.io import *\n",
    "from chrisbase.util import *\n",
    "\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "\n",
    "def md(t):\n",
    "    display(Markdown(t))\n",
    "\n",
    "\n",
    "def hr(s=\"<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>\"):\n",
    "    display(HTML(s))\n",
    "\n",
    "\n",
    "def html(t,\n",
    "         s=\"<pre style='font-family:Arial; font-size:12pt;\"\n",
    "           \" padding:10px; margin-left:10px; margin-top:10px;\"\n",
    "           \" background-color:LightSkyBlue; border:3px solid MidnightBlue;\"\n",
    "           \" width:100%; height:100px;'>\",\n",
    "         e=\"</pre>\"):\n",
    "    display(HTML(s + t + e))\n",
    "\n",
    "\n",
    "def llama3_family():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        llama-3 --> llama-3-8b\n",
    "        llama-3 --> llama-3-70b\n",
    "        llama-3-8b --> llama-3-8b-instruct\n",
    "        llama-3-70b --> llama-3-70b-instruct\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def genai_app_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[Users] --> B(Applications e.g. mobile, web)\n",
    "        B --> |Hosted API|C(Platforms e.g. Custom, HuggingFace, Replicate)\n",
    "        B -- optional --> E(Frameworks e.g. LangChain)\n",
    "        C-->|User Input|D[Llama 3]\n",
    "        D-->|Model Output|C\n",
    "        E --> C\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def bot_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        user --> prompt\n",
    "        prompt --> i_safety\n",
    "        i_safety --> context\n",
    "        context --> Llama_3\n",
    "        Llama_3 --> output\n",
    "        output --> o_safety\n",
    "        i_safety --> memory\n",
    "        o_safety --> memory\n",
    "        memory --> context\n",
    "        o_safety --> user\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def mem_context():\n",
    "    mm(\"\"\"\n",
    "    graph LR\n",
    "        context(text)\n",
    "        user_prompt --> context\n",
    "        instruction --> context\n",
    "        examples --> context\n",
    "        memory --> context\n",
    "        context --> tokenizer\n",
    "        tokenizer --> embeddings\n",
    "        embeddings --> LLM\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def rag_arch():\n",
    "    mm(\"\"\"\n",
    "    flowchart TD\n",
    "        A[User Prompts] --> B(Frameworks e.g. LangChain)\n",
    "        B <--> |Database, Docs, XLS|C[fa:fa-database External Data]\n",
    "        B -->|API|D[Llama 3]\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def load_data_faiss_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        documents --> textsplitter\n",
    "        textsplitter --> embeddings\n",
    "        embeddings --> vectorstore\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def fine_tuned_arch():\n",
    "    mm(\"\"\"\n",
    "    graph LR;\n",
    "        Custom_Dataset --> Pre-trained_Llama\n",
    "        Pre-trained_Llama --> Fine-tuned_Llama\n",
    "        Fine-tuned_Llama --> RLHF\n",
    "        RLHF --> |Loss:Cross-Entropy|Fine-tuned_Llama\n",
    "        classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "    \"\"\")"
   ],
   "outputs": [],
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "id": "acd794f15ce1f743",
   "metadata": {},
   "source": "### **1 - Understanding Llama 3**"
  },
  {
   "cell_type": "markdown",
   "id": "548692b6a6dd66c",
   "metadata": {},
   "source": [
    "### **1.1 - What is Llama 3?**\n",
    "\n",
    "* State of the art (SOTA), Open Source LLM\n",
    "* 8B, 70B\n",
    "* Choosing model: Size, Quality, Cost, Speed\n",
    "* Pretrained + Chat\n",
    "* [Meta Llama 3 Blog](https://ai.meta.com/blog/meta-llama-3/)\n",
    "* [Getting Started with Meta Llama](https://llama.meta.com/docs/get-started)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:15.668428Z",
     "start_time": "2024-05-02T08:47:15.666275Z"
    }
   },
   "cell_type": "code",
   "source": "llama3_family()",
   "id": "a2ef68ece727fa3f",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBsbGFtYS0zIC0tPiBsbGFtYS0zLThiCiAgICAgICAgbGxhbWEtMyAtLT4gbGxhbWEtMy03MGIKICAgICAgICBsbGFtYS0zLThiIC0tPiBsbGFtYS0zLThiLWluc3RydWN0CiAgICAgICAgbGxhbWEtMy03MGIgLS0+IGxsYW1hLTMtNzBiLWluc3RydWN0CiAgICAgICAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAgICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "id": "190b25e8cdc4c64d",
   "metadata": {},
   "source": [
    "### **1.2 - Accessing Llama 3**\n",
    "* Download + Self Host (i.e. [download Llama](https://ai.meta.com/resources/models-and-libraries/llama-downloads))\n",
    "* Hosted API Platform (e.g. [Groq](https://console.groq.com/), [Replicate](https://replicate.com/meta/meta-llama-3-8b-instruct), [Together](https://api.together.xyz/playground/language/meta-llama/Llama-3-8b-hf), [Anyscale](https://app.endpoints.anyscale.com/playground))\n",
    "* Hosted Container Platform (e.g. [Azure](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-llama-2-on-azure/ba-p/3881233), [AWS](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/), [GCP](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0f1cf7c500be4",
   "metadata": {},
   "source": [
    "### **1.3 - Use Cases of Llama 3**\n",
    "* Content Generation\n",
    "* Summarization\n",
    "* General Chatbots\n",
    "* RAG (Retrieval Augmented Generation): Chat about Your Own Data\n",
    "* Fine-tuning\n",
    "* Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965614383680fb8",
   "metadata": {},
   "source": [
    "## **2 - Using and Comparing Llama 3 and Llama 2**\n",
    "\n",
    "In this notebook, we will use the Llama 2 70b chat and Llama 3 8b and 70b instruct models hosted on [Groq](https://console.groq.com/). You'll need to first [sign in](https://console.groq.com/) with your github or gmail account, then get an [API token](https://console.groq.com/keys) to try Groq out for free. (Groq runs Llama models very fast and they only support one Llama 2 model: the Llama 2 70b chat).\n",
    "\n",
    "**Note: You can also use other Llama hosting providers such as [Replicate](https://replicate.com/blog/run-llama-3-with-an-api?input=python), [Togther](https://docs.together.ai/docs/quickstart). Simply click the links here to see how to run `pip install` and use their freel trial API key with example code to modify the following three cells in 2.1 and 2.2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1a112c1342b73",
   "metadata": {},
   "source": [
    "### **2.1 - Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a77a8bb71e58ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:16.258345Z",
     "start_time": "2024-05-02T08:47:15.669272Z"
    }
   },
   "source": [
    "# Install dependencies and initialize\n",
    "!pip list | grep -E \"langchain|transformers|torch|faiss|groq|replicate|chris\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrisbase                 0.5.2\r\n",
      "faiss-cpu                 1.8.0\r\n",
      "groq                      0.5.0\r\n",
      "langchain                 0.1.17\r\n",
      "langchain-community       0.0.36\r\n",
      "langchain-core            0.1.48\r\n",
      "langchain-text-splitters  0.0.1\r\n",
      "replicate                 0.25.2\r\n",
      "sentence-transformers     2.7.0\r\n",
      "torch                     2.3.0\r\n",
      "transformers              4.40.1\r\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.2 - Create helpers for Llama 2 and Llama 3**\n",
    "First, set your Groq API token as environment variables."
   ],
   "id": "da1eb02f1aa9b632"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:16.272763Z",
     "start_time": "2024-05-02T08:47:16.259880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GROQ_API_TOKEN = read_or(first_path_or(\".groq*.key\")) or getpass()\n",
    "REPLICATE_API_TOKEN = read_or(first_path_or(\".replicate*.key\")) or getpass()\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_TOKEN\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "print(f\"Groq API Key: {mask_str(GROQ_API_TOKEN, start=4, end=-4)}\")\n",
    "print(f\"Replicate API Key: {mask_str(REPLICATE_API_TOKEN, start=3, end=-3)}\")\n",
    "\n",
    "logging.getLogger(\"IPKernelApp\").setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "args = CommonArguments(\n",
    "    env=ProjectEnv(\n",
    "        project=\"LLM-based\",\n",
    "        job_name=\"LLaMA-2-13B-Chat\",\n",
    "        msg_level=logging.INFO,\n",
    "        msg_format=LoggingFormat.BRIEF_00,\n",
    "    )\n",
    ")\n",
    "args.dataframe()"
   ],
   "id": "3f33f0fc379c4f04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API Key: gsk_************************************************016r\n",
      "Replicate API Key: r8_**********************************MYA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      CommonArguments                                                                                        value\n",
       "0                 tag                                                                                         None\n",
       "1         env.project                                                                                    LLM-based\n",
       "2        env.job_name                                                                             LLaMA-2-13B-Chat\n",
       "3     env.job_version                                                                                         None\n",
       "4        env.hostname                                                                           ChrisBookPro.local\n",
       "5        env.hostaddr                                                                                  192.168.0.8\n",
       "6      env.time_stamp                                                                                  0502.172257\n",
       "7     env.python_path                                        /Users/chris/miniforge3/envs/LLM-based/bin/python3.11\n",
       "8     env.current_dir                                                                  /Users/chris/proj/LLM-based\n",
       "9    env.current_file                                    /Users/chris/proj/LLM-based/Getting_to_know_Llama_3.ipynb\n",
       "10    env.working_dir                                                                  /Users/chris/proj/LLM-based\n",
       "11   env.command_args  [-f, /Users/chris/Library/Jupyter/runtime/kernel-93f690f1-151a-4d13-a85f-d65f624bb84c.json]\n",
       "12   env.num_ip_addrs                                                                                            0\n",
       "13    env.max_workers                                                                                            1\n",
       "14      env.debugging                                                                                        False\n",
       "15      env.msg_level                                                                                           20\n",
       "16     env.msg_format                                                                    %(asctime)s ┇ %(message)s\n",
       "17    env.date_format                                                                             [%m.%d %H:%M:%S]\n",
       "18    env.output_home                                                                                         None\n",
       "19   env.logging_file                                                                                         None\n",
       "20  env.argument_file                                                                                         None\n",
       "21       time.started                                                                                         None\n",
       "22       time.settled                                                                                         None\n",
       "23       time.elapsed                                                                                         None"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommonArguments</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tag</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>env.project</td>\n",
       "      <td>LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>env.job_name</td>\n",
       "      <td>LLaMA-2-13B-Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>env.job_version</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>env.hostname</td>\n",
       "      <td>ChrisBookPro.local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>env.hostaddr</td>\n",
       "      <td>192.168.0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>env.time_stamp</td>\n",
       "      <td>0502.172257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>env.python_path</td>\n",
       "      <td>/Users/chris/miniforge3/envs/LLM-based/bin/python3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>env.current_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>env.current_file</td>\n",
       "      <td>/Users/chris/proj/LLM-based/Getting_to_know_Llama_3.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>env.working_dir</td>\n",
       "      <td>/Users/chris/proj/LLM-based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>env.command_args</td>\n",
       "      <td>[-f, /Users/chris/Library/Jupyter/runtime/kernel-93f690f1-151a-4d13-a85f-d65f624bb84c.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>env.num_ip_addrs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>env.max_workers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>env.debugging</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>env.msg_level</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>env.msg_format</td>\n",
       "      <td>%(asctime)s ┇ %(message)s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>env.date_format</td>\n",
       "      <td>[%m.%d %H:%M:%S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>env.output_home</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>env.logging_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>env.argument_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>time.started</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>time.settled</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>time.elapsed</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create Llama 2 and Llama 3 helper functions - for chatbot type of apps, we'll use Llama 3 8b/70b instruct models, not the base models.",
   "id": "e60f5e428045f58b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:16.284411Z",
     "start_time": "2024-05-02T08:47:16.273618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "def llama2_13b(prompt, system_prompt=None):\n",
    "    model_ref = \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\"\n",
    "    api_input = {\n",
    "        \"prompt\": prompt,\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"max_new_tokens\": 1000,\n",
    "    }\n",
    "    if system_prompt is None:\n",
    "        api_input.pop(\"system_prompt\")\n",
    "    out = replicate.run(model_ref, input=api_input)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "def llama2_70b(prompt, system_prompt=None):\n",
    "    model_ref = \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\"\n",
    "    api_input = {\n",
    "        \"prompt\": prompt,\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"max_new_tokens\": 1000,\n",
    "    }\n",
    "    if system_prompt is None:\n",
    "        api_input.pop(\"system_prompt\")\n",
    "    out = replicate.run(model_ref, input=api_input)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "def llama3_8b(prompt, temperature=0.0, input_print=True):\n",
    "    model_ref = \"llama3-8b-8192\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model_ref,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def llama3_70b(prompt, temperature=0.0, input_print=True):\n",
    "    model_ref = \"llama3-70b-8192\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model_ref,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def gemma(prompt, temperature=0.0, input_print=True):\n",
    "    model_ref = \"gemma-7b-it\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model_ref,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def mixtral(prompt, temperature=0.0, input_print=True):\n",
    "    model_ref = \"mixtral-8x7b-32768\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model_ref,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ],
   "id": "bb7eba6f7af15ebc",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.3 - Basic QA with Llama 2 and 3**",
   "id": "6b3e0c0e9cf2a4a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:20.668445Z",
     "start_time": "2024-05-02T08:47:16.285699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"2.3 - Basic QA with Llama 2 and 3 [1/5]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output1 = llama2_13b(\"The typical color of a llama is: \")\n",
    "    output2 = llama2_13b(\"The typical color of a llama is: \", system_prompt=\"respond with only one word\")\n",
    "hr()\n",
    "md(output1)\n",
    "hr()\n",
    "md(output2)\n",
    "hr()"
   ],
   "id": "ee8dac91906817bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.02 17:47:16] ┇ ================================================================================\n",
      "[05.02 17:47:16] ┇ [INIT] 2.3 - Basic QA with Llama 2 and 3 [1/5]\n",
      "[05.02 17:47:16] ┇ ================================================================================\n",
      "[05.02 17:47:17] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.02 17:47:17] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:18] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/fm8ntx4a3hrgp0cf71y8ttm1f4 \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:18] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/fm8ntx4a3hrgp0cf71y8ttm1f4 \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:19] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.02 17:47:19] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:20] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/v45pg1wjhhrgj0cf71y9y9xt6w \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:20] ┇ ================================================================================\n",
      "[05.02 17:47:20] ┇ [EXIT] 2.3 - Basic QA with Llama 2 and 3 [1/5] ($=00:00:03.535)\n",
      "[05.02 17:47:20] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": " Hello! I'm here to help answer your questions safely and helpfully. The typical color of a llama is a beautiful shade of brown, ranging from a light beige to a dark chocolate brown. Some llamas may also have white markings on their faces or legs, but overall, the typical color is a warm, rich brown. I hope that helps! Is there anything else you'd like to know?"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": " Sure! Here's my response:\n\nBrown."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:22.854019Z",
     "start_time": "2024-05-02T08:47:20.669276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"2.3 - Basic QA with Llama 2 and 3 [2/5]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output1 = llama3_8b(\"The typical color of a llama is: \")\n",
    "    output2 = llama3_8b(\"The typical color of a llama is what? Answer in one word.\")\n",
    "hr()\n",
    "md(output1)\n",
    "hr()\n",
    "md(output2)\n",
    "hr()"
   ],
   "id": "6bdcbc2d954fd365",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.02 17:47:20] ┇ ================================================================================\n",
      "[05.02 17:47:20] ┇ [INIT] 2.3 - Basic QA with Llama 2 and 3 [2/5]\n",
      "[05.02 17:47:20] ┇ ================================================================================\n",
      "[05.02 17:47:22] ┇ HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:22] ┇ HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:22] ┇ ================================================================================\n",
      "[05.02 17:47:22] ┇ [EXIT] 2.3 - Basic QA with Llama 2 and 3 [2/5] ($=00:00:01.351)\n",
      "[05.02 17:47:22] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The typical color of a llama is white! However, llamas can also come in a variety of other colors, including:\n\n* Suri: a soft, fluffy coat that can be white, cream, or light brown\n* Huacaya: a dense, soft coat that can be white, cream, or various shades of brown, gray, or black\n* Rose-gray: a light grayish-pink color\n* Dark brown: a rich, dark brown color\n* Black: a glossy black coat\n* Bay: a reddish-brown color\n* Chestnut: a reddish-brown color with a darker mane and tail\n* Cream: a light cream or beige color\n* Fawn: a light reddish-brown color with a darker mane and tail\n\nIt's worth noting that llamas can also have various markings, such as white patches or stripes, on their coats."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Brown."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:24.637386Z",
     "start_time": "2024-05-02T08:47:22.854893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"2.3 - Basic QA with Llama 2 and 3 [3/5]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output1 = llama3_70b(\"The typical color of a llama is: \")\n",
    "    output2 = llama3_70b(\"The typical color of a llama is what? Answer in one word.\")\n",
    "hr()\n",
    "md(output1)\n",
    "hr()\n",
    "md(output2)\n",
    "hr()"
   ],
   "id": "5e4c87f562db855",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.02 17:47:23] ┇ ================================================================================\n",
      "[05.02 17:47:23] ┇ [INIT] 2.3 - Basic QA with Llama 2 and 3 [3/5]\n",
      "[05.02 17:47:23] ┇ ================================================================================\n",
      "[05.02 17:47:23] ┇ HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:24] ┇ HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:24] ┇ ================================================================================\n",
      "[05.02 17:47:24] ┇ [EXIT] 2.3 - Basic QA with Llama 2 and 3 [3/5] ($=00:00:00.943)\n",
      "[05.02 17:47:24] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Llamas can come in a variety of colors, but the most common colors are:\n\n* Suri llamas: white, light brown, dark brown, gray, and black\n* Huacaya llamas: white, beige, brown, gray, black, and various shades of red and roan (a mix of white and dark hairs)\n\nSome llamas can also have markings such as white or dark patches on their faces, legs, or bodies.\n\nSo, there isn't just one \"typical\" color for llamas, as they can come in a range of colors and patterns!"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Beige."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:26.238098Z",
     "start_time": "2024-05-02T08:47:24.638538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"2.3 - Basic QA with Llama 2 and 3 [4/5]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output1 = gemma(\"The typical color of a llama is: \")\n",
    "    output2 = gemma(\"The typical color of a llama is what? Answer in one word.\")\n",
    "hr()\n",
    "md(output1)\n",
    "hr()\n",
    "md(output2)\n",
    "hr()"
   ],
   "id": "bd1a81cbc7c25bd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.02 17:47:24] ┇ ================================================================================\n",
      "[05.02 17:47:24] ┇ [INIT] 2.3 - Basic QA with Llama 2 and 3 [4/5]\n",
      "[05.02 17:47:24] ┇ ================================================================================\n",
      "[05.02 17:47:25] ┇ HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:25] ┇ HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:26] ┇ ================================================================================\n",
      "[05.02 17:47:26] ┇ [EXIT] 2.3 - Basic QA with Llama 2 and 3 [4/5] ($=00:00:00.755)\n",
      "[05.02 17:47:26] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Brown or gray**\n\nLlamas are known for their distinctive brown or gray coats, which provide insulation and protection from the elements."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Brown"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:47:27.939299Z",
     "start_time": "2024-05-02T08:47:26.238796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with JobTimer(\"2.3 - Basic QA with Llama 2 and 3 [5/5]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output1 = mixtral(\"The typical color of a llama is: \")\n",
    "    output2 = mixtral(\"The typical color of a llama is what? Answer in one word.\")\n",
    "hr()\n",
    "md(output1)\n",
    "hr()\n",
    "md(output2)\n",
    "hr()"
   ],
   "id": "305488e462bb3f11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.02 17:47:26] ┇ ================================================================================\n",
      "[05.02 17:47:26] ┇ [INIT] 2.3 - Basic QA with Llama 2 and 3 [5/5]\n",
      "[05.02 17:47:26] ┇ ================================================================================\n",
      "[05.02 17:47:27] ┇ HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:27] ┇ HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[05.02 17:47:27] ┇ ================================================================================\n",
      "[05.02 17:47:27] ┇ [EXIT] 2.3 - Basic QA with Llama 2 and 3 [5/5] ($=00:00:00.861)\n",
      "[05.02 17:47:27] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The typical color of a llama can vary, but they are often seen in shades of brown, black, and white. Some llamas can be a mix of these colors, resulting in a wide range of patterns and hues. There are also certain less common colors, such as gray or roan, that can be seen in llamas. Overall, the color of a llama can be quite variable and is not strictly limited to a single shade."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Brown\n\nWhile there is no \"typical\" color as llamas can be found in many colors, brown is the most common coat color for these animals. Other possible colors include white, black, grey, and various shades of tan or beige. Some llamas even have multi-colored coats with patches of different colors."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<hr style='border:3px solid MidnightBlue; width:880px; padding:0; margin:0; margin-bottom:15px; margin-top:10px;'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3 - Gen AI Application Architecture**\n",
    "\n",
    "Here is the high-level tech stack/architecture of Generative AI application."
   ],
   "id": "9a0a627dd9fdb44c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:21.602759Z",
     "start_time": "2024-05-01T05:57:21.600218Z"
    }
   },
   "cell_type": "code",
   "source": "genai_app_arch()",
   "id": "b7175c90b89a9cac",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBmbG93Y2hhcnQgVEQKICAgICAgICBBW1VzZXJzXSAtLT4gQihBcHBsaWNhdGlvbnMgZS5nLiBtb2JpbGUsIHdlYikKICAgICAgICBCIC0tPiB8SG9zdGVkIEFQSXxDKFBsYXRmb3JtcyBlLmcuIEN1c3RvbSwgSHVnZ2luZ0ZhY2UsIFJlcGxpY2F0ZSkKICAgICAgICBCIC0tIG9wdGlvbmFsIC0tPiBFKEZyYW1ld29ya3MgZS5nLiBMYW5nQ2hhaW4pCiAgICAgICAgQy0tPnxVc2VyIElucHV0fERbTGxhbWEgMl0KICAgICAgICBELS0+fE1vZGVsIE91dHB1dHxDCiAgICAgICAgRSAtLT4gQwogICAgICAgIGNsYXNzRGVmIGRlZmF1bHQgZmlsbDojQ0NFNkZGLHN0cm9rZTojODRCQ0Y1LHRleHRDb2xvcjojMUMyQjMzLGZvbnRGYW1pbHk6dHJlYnVjaGV0IG1zOwogICAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **4 - Chatbot Architecture**\n",
    "\n",
    "Here are the key components and the information flow in a chatbot.\n",
    "* User Prompts\n",
    "* Input Safety\n",
    "* Llama 2\n",
    "* Output Safety\n",
    "* Memory & Context"
   ],
   "id": "7ca1709c6ac20994"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:21.605818Z",
     "start_time": "2024-05-01T05:57:21.603556Z"
    }
   },
   "cell_type": "code",
   "source": "bot_arch()",
   "id": "daab8e849bb8fdb2",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICB1c2VyIC0tPiBwcm9tcHQKICAgICAgICBwcm9tcHQgLS0+IGlfc2FmZXR5CiAgICAgICAgaV9zYWZldHkgLS0+IGNvbnRleHQKICAgICAgICBjb250ZXh0IC0tPiBMbGFtYV8yCiAgICAgICAgTGxhbWFfMiAtLT4gb3V0cHV0CiAgICAgICAgb3V0cHV0IC0tPiBvX3NhZmV0eQogICAgICAgIGlfc2FmZXR5IC0tPiBtZW1vcnkKICAgICAgICBvX3NhZmV0eSAtLT4gbWVtb3J5CiAgICAgICAgbWVtb3J5IC0tPiBjb250ZXh0CiAgICAgICAgb19zYWZldHkgLS0+IHVzZXIKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **4.1 - Chat conversation**\n",
    "* LLMs are stateless\n",
    "* Single Turn\n",
    "* Multi Turn (Memory)"
   ],
   "id": "7cafbf05e3b4c9c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:24.603236Z",
     "start_time": "2024-05-01T05:57:21.606581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of single turn chat\n",
    "prompt_chat = \"What is the average lifespan of a Llama?\"\n",
    "with JobTimer(\"4.1 - Chat conversation: Single Turn [1/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question in few words\")\n",
    "html(output)"
   ],
   "id": "ce5be38fb3a6de58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:21] ┇ ================================================================================\n",
      "[05.01 14:57:21] ┇ [INIT] 4.1 - Chat conversation: Single Turn [1/2]\n",
      "[05.01 14:57:21] ┇ ================================================================================\n",
      "[05.01 14:57:22] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:57:23] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:24] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/x90824mz1xrgm0cf6axas1r19c \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:24] ┇ ================================================================================\n",
      "[05.01 14:57:24] ┇ [EXIT] 4.1 - Chat conversation: Single Turn [1/2] ($=00:00:02.161)\n",
      "[05.01 14:57:24] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! The average lifespan of a llama is around 15-20 years.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:27.577684Z",
     "start_time": "2024-05-01T05:57:24.604023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example without previous context. LLM's are stateless and cannot understand \"they\" without previous context\n",
    "prompt_chat = \"What animal family are they?\"\n",
    "with JobTimer(\"4.1 - Chat conversation: Single Turn [2/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question in few words\")\n",
    "html(output)"
   ],
   "id": "8530c9c8b79f9eaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:24] ┇ ================================================================================\n",
      "[05.01 14:57:24] ┇ [INIT] 4.1 - Chat conversation: Single Turn [2/2]\n",
      "[05.01 14:57:24] ┇ ================================================================================\n",
      "[05.01 14:57:25] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:57:25] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:27] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/s77nkjdahxrgp0cf6axbmq2v04 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:27] ┇ ================================================================================\n",
      "[05.01 14:57:27] ┇ [EXIT] 4.1 - Chat conversation: Single Turn [2/2] ($=00:00:02.138)\n",
      "[05.01 14:57:27] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Here's the answer in a few words:\n",
       "\n",
       "They are from the canine family.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Chat app requires us to send in previous context to LLM to get in valid responses. Below is an example of Multi-turn chat.",
   "id": "5ed6c2e295ea021b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:27.581042Z",
     "start_time": "2024-05-01T05:57:27.578604Z"
    }
   },
   "cell_type": "code",
   "source": "mem_context()",
   "id": "dc83a7bc22016551",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUgogICAgICAgIGNvbnRleHQodGV4dCkKICAgICAgICB1c2VyX3Byb21wdCAtLT4gY29udGV4dAogICAgICAgIGluc3RydWN0aW9uIC0tPiBjb250ZXh0CiAgICAgICAgZXhhbXBsZXMgLS0+IGNvbnRleHQKICAgICAgICBtZW1vcnkgLS0+IGNvbnRleHQKICAgICAgICBjb250ZXh0IC0tPiB0b2tlbml6ZXIKICAgICAgICB0b2tlbml6ZXIgLS0+IGVtYmVkZGluZ3MKICAgICAgICBlbWJlZGRpbmdzIC0tPiBMTE0KICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:29.843414Z",
     "start_time": "2024-05-01T05:57:27.583656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of multi-turn chat, with storing previous context\n",
    "prompt_chat = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: Sure! The average lifespan of a llama is around 20-30 years.\n",
    "User: What animal family are they?\n",
    "\"\"\"\n",
    "with JobTimer(\"4.1 - Chat conversation: Multi Turn (Memory)\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question\")\n",
    "html(output)"
   ],
   "id": "8454cb422834ea24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:27] ┇ ================================================================================\n",
      "[05.01 14:57:27] ┇ [INIT] 4.1 - Chat conversation: Multi Turn (Memory)\n",
      "[05.01 14:57:27] ┇ ================================================================================\n",
      "[05.01 14:57:28] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:57:28] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:29] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/s9fah0np81rgp0cf6ax9dypdeg \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:29] ┇ ================================================================================\n",
      "[05.01 14:57:29] ┇ [EXIT] 4.1 - Chat conversation: Multi Turn (Memory) ($=00:00:01.416)\n",
      "[05.01 14:57:29] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Llamas are members of the camelid family, which includes camels, alpacas, and vicuñas.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **4.2 - Prompt Engineering**\n",
    "* Prompt engineering refers to the science of designing effective prompts to get desired responses\n",
    "* Helps reduce hallucination"
   ],
   "id": "5ae6ff4bba0e4d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **4.2.1 - In-Context Learning (e.g. Zero-shot, Few-shot)**\n",
    "  * In-context learning - specific method of prompt engineering where demonstration of task are provided as part of prompt.\n",
    "  1. Zero-shot learning - model is performing tasks without any input examples.\n",
    "  2. Few or “N-Shot” Learning - model is performing and behaving based on input examples in user's prompt."
   ],
   "id": "195ccff05618fb37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:32.501864Z",
     "start_time": "2024-05-01T05:57:29.844094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Zero-shot example. To get positive/negative/neutral sentiment, we need to give examples in the prompt\n",
    "prompt = '''\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment: ?\n",
    "'''\n",
    "with JobTimer(\"4.2.1 - In-Context Learning [1/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"one word response\")\n",
    "html(output)"
   ],
   "id": "29b2d6ac41db8de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:30] ┇ ================================================================================\n",
      "[05.01 14:57:30] ┇ [INIT] 4.2.1 - In-Context Learning [1/4]\n",
      "[05.01 14:57:30] ┇ ================================================================================\n",
      "[05.01 14:57:30] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:57:30] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:32] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/79d91ddzaxrgg0cf6axb6cg11m \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:32] ┇ ================================================================================\n",
      "[05.01 14:57:32] ┇ [EXIT] 4.2.1 - In-Context Learning [1/4] ($=00:00:01.808)\n",
      "[05.01 14:57:32] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Cute</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:36.183512Z",
     "start_time": "2024-05-01T05:57:32.503344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By giving examples to Llama, it understands the expected output format.\n",
    "prompt = '''\n",
    "Classify: I love Llamas!\n",
    "Sentiment: Positive\n",
    "Classify: I dont like Snakes.\n",
    "Sentiment: Negative\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment:'''\n",
    "with JobTimer(\"4.2.1 - In-Context Learning [2/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"One word response\")\n",
    "html(output)"
   ],
   "id": "82e85255a9c01c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:32] ┇ ================================================================================\n",
      "[05.01 14:57:32] ┇ [INIT] 4.2.1 - In-Context Learning [2/4]\n",
      "[05.01 14:57:32] ┇ ================================================================================\n",
      "[05.01 14:57:33] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:57:34] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:35] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/c9yh8ny9tnrgj0cf6axaqkzxcg \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:35] ┇ ================================================================================\n",
      "[05.01 14:57:35] ┇ [EXIT] 4.2.1 - In-Context Learning [2/4] ($=00:00:02.830)\n",
      "[05.01 14:57:35] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Neutral</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:39.878327Z",
     "start_time": "2024-05-01T05:57:36.185443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# another zero-shot learning\n",
    "prompt = '''\n",
    "QUESTION: Vicuna?\n",
    "ANSWER:'''\n",
    "with JobTimer(\"4.2.1 - In-Context Learning [3/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"one word response\")\n",
    "html(output)"
   ],
   "id": "101bb2a57686a5f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:36] ┇ ================================================================================\n",
      "[05.01 14:57:36] ┇ [INIT] 4.2.1 - In-Context Learning [3/4]\n",
      "[05.01 14:57:36] ┇ ================================================================================\n",
      "[05.01 14:57:37] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:57:38] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:39] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/jcj4rwytf1rgp0cf6ax947c5t0 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:39] ┇ ================================================================================\n",
      "[05.01 14:57:39] ┇ [EXIT] 4.2.1 - In-Context Learning [3/4] ($=00:00:02.839)\n",
      "[05.01 14:57:39] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Luxurious.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:43.860092Z",
     "start_time": "2024-05-01T05:57:39.880171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Another few-shot learning example with formatted prompt.\n",
    "prompt = '''\n",
    "QUESTION: Llama?\n",
    "ANSWER: Yes\n",
    "QUESTION: Alpaca?\n",
    "ANSWER: Yes\n",
    "QUESTION: Rabbit?\n",
    "ANSWER: No\n",
    "QUESTION: Vicuna?\n",
    "ANSWER:\n",
    "'''\n",
    "with JobTimer(\"4.2.1 - In-Context Learning [4/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"one word response\")\n",
    "html(output)"
   ],
   "id": "e202cd8ba912d8cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:40] ┇ ================================================================================\n",
      "[05.01 14:57:40] ┇ [INIT] 4.2.1 - In-Context Learning [4/4]\n",
      "[05.01 14:57:40] ┇ ================================================================================\n",
      "[05.01 14:57:41] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:57:41] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:43] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/r2rbzaq6khrgg0cf6axajp78qw \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:43] ┇ ================================================================================\n",
      "[05.01 14:57:43] ┇ [EXIT] 4.2.1 - In-Context Learning [4/4] ($=00:00:03.135)\n",
      "[05.01 14:57:43] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Here's my one-word answer:\n",
       "\n",
       "Yes</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **4.2.2 - Chain of Thought**\n",
    "\"Chain of thought\" enables complex reasoning through logical step by step thinking and generates meaningful and contextually relevant responses."
   ],
   "id": "8c0eb01f03c32d83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:46.486971Z",
     "start_time": "2024-05-01T05:57:43.861673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standard prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Llama have now?\n",
    "'''\n",
    "with JobTimer(\"4.2.2 - Chain of Thought [1/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"provide short answer\")\n",
    "html(output)"
   ],
   "id": "76484a2702ce9cf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:44] ┇ ================================================================================\n",
      "[05.01 14:57:44] ┇ [INIT] 4.2.2 - Chain of Thought [1/2]\n",
      "[05.01 14:57:44] ┇ ================================================================================\n",
      "[05.01 14:57:44] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:57:45] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:46] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/z1j0qsznsdrgm0cf6ax9jzhh4m \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:46] ┇ ================================================================================\n",
      "[05.01 14:57:46] ┇ [EXIT] 4.2.2 - Chain of Thought [1/2] ($=00:00:01.782)\n",
      "[05.01 14:57:46] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Here's the answer to your question:\n",
       "\n",
       "Llama has 5 + 2 x 3 = 5 + 6 = 11 tennis balls now.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:48.947066Z",
     "start_time": "2024-05-01T05:57:46.488596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chain-Of-Thought prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Llama have now?\n",
    "Let's think step by step.\n",
    "'''\n",
    "with JobTimer(\"4.2.2 - Chain of Thought [2/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    output = ChatCompletion(prompt, system_prompt=\"provide short answer\")\n",
    "html(output)"
   ],
   "id": "c1aaa17e88044f92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:46] ┇ ================================================================================\n",
      "[05.01 14:57:46] ┇ [INIT] 4.2.2 - Chain of Thought [2/2]\n",
      "[05.01 14:57:46] ┇ ================================================================================\n",
      "[05.01 14:57:47] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:57:47] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:48] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/w9j11yr0cnrgj0cf6axrpx1a2r \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:57:48] ┇ ================================================================================\n",
      "[05.01 14:57:48] ┇ [EXIT] 4.2.2 - Chain of Thought [2/2] ($=00:00:01.613)\n",
      "[05.01 14:57:48] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure! Here's the solution:\n",
       "\n",
       "Llama started with 5 tennis balls.\n",
       "It buys 2 more cans of tennis balls.\n",
       "Each can has 3 tennis balls.\n",
       "\n",
       "So, Llama buys 2 x 3 = 6 tennis balls.\n",
       "\n",
       "Now, Llama has 5 + 6 = 11 tennis balls.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **4.3 - Retrieval Augmented Generation (RAG)**\n",
    "* Prompt Eng Limitations - Knowledge cutoff & lack of specialized data\n",
    "* Retrieval Augmented Generation(RAG) allows us to retrieve snippets of information from external data sources and augment it to the user's prompt to get tailored responses from Llama 2.\n",
    "\n",
    "For our demo, we are going to download an external PDF file from a URL and query against the content in the pdf file to get contextually relevant information back with the help of Llama!"
   ],
   "id": "2c87637f7e017552"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:48.949977Z",
     "start_time": "2024-05-01T05:57:48.947844Z"
    }
   },
   "cell_type": "code",
   "source": "rag_arch()",
   "id": "17bd3c2b24aa01cc",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBmbG93Y2hhcnQgVEQKICAgICAgICBBW1VzZXIgUHJvbXB0c10gLS0+IEIoRnJhbWV3b3JrcyBlLmcuIExhbmdDaGFpbikKICAgICAgICBCIDwtLT4gfERhdGFiYXNlLCBEb2NzLCBYTFN8Q1tmYTpmYS1kYXRhYmFzZSBFeHRlcm5hbCBEYXRhXQogICAgICAgIEIgLS0+fEFQSXxEW0xsYW1hIDJdCiAgICAgICAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAgICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **4.3.1 - LangChain**\n",
    "LangChain is a framework that helps make it easier to implement RAG."
   ],
   "id": "e41075a070214735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:48.952939Z",
     "start_time": "2024-05-01T05:57:48.950940Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_faiss_arch()",
   "id": "99da4d3309a10773",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBkb2N1bWVudHMgLS0+IHRleHRzcGxpdHRlcgogICAgICAgIHRleHRzcGxpdHRlciAtLT4gZW1iZWRkaW5ncwogICAgICAgIGVtYmVkZGluZ3MgLS0+IHZlY3RvcnN0b3JlCiAgICAgICAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAgICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:54.574904Z",
     "start_time": "2024-05-01T05:57:48.953577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: load the external data source. In our case, we will load Meta’s “Responsible Use Guide” pdf document.\n",
    "with JobTimer(\"4.3.1 - LangChain [1/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    loader = OnlinePDFLoader(\"https://ai.meta.com/static-resource/responsible-use-guide/\")\n",
    "    document = loader.load()\n",
    "md(f\"{type(document)} of {type(document[0])}\")"
   ],
   "id": "442bef330ddd6394",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:49] ┇ ================================================================================\n",
      "[05.01 14:57:49] ┇ [INIT] 4.3.1 - LangChain [1/4]\n",
      "[05.01 14:57:49] ┇ ================================================================================\n",
      "[05.01 14:57:52] ┇ pikepdf C++ to Python logger bridge initialized\n",
      "[05.01 14:57:54] ┇ ================================================================================\n",
      "[05.01 14:57:54] ┇ [EXIT] 4.3.1 - LangChain [1/4] ($=00:00:04.785)\n",
      "[05.01 14:57:54] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<class 'list'> of <class 'langchain_core.documents.base.Document'>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:57:55.422940Z",
     "start_time": "2024-05-01T05:57:54.576508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Get text splits from document\n",
    "with JobTimer(\"4.3.1 - LangChain [2/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "    all_splits = text_splitter.split_documents(document)\n",
    "md(f\"{type(all_splits)} of {type(all_splits[0])}\")"
   ],
   "id": "9fad448a7595cc9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:54] ┇ ================================================================================\n",
      "[05.01 14:57:54] ┇ [INIT] 4.3.1 - LangChain [2/4]\n",
      "[05.01 14:57:54] ┇ ================================================================================\n",
      "[05.01 14:57:55] ┇ ================================================================================\n",
      "[05.01 14:57:55] ┇ [EXIT] 4.3.1 - LangChain [2/4] ($=00:00:00.002)\n",
      "[05.01 14:57:55] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<class 'list'> of <class 'langchain_core.documents.base.Document'>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:58:00.167819Z",
     "start_time": "2024-05-01T05:57:55.423565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Use the embedding model\n",
    "with JobTimer(\"4.3.1 - LangChain [3/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"  # embedding model\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    model_embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "md(f\"{type(model_embeddings)}\")"
   ],
   "id": "59dd6ff691df3bf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:57:55] ┇ ================================================================================\n",
      "[05.01 14:57:55] ┇ [INIT] 4.3.1 - LangChain [3/4]\n",
      "[05.01 14:57:55] ┇ ================================================================================\n",
      "[05.01 14:57:56] ┇ Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "[05.01 14:57:59] ┇ ================================================================================\n",
      "[05.01 14:57:59] ┇ [EXIT] 4.3.1 - LangChain [3/4] ($=00:00:03.904)\n",
      "[05.01 14:57:59] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<class 'langchain_community.embeddings.huggingface.HuggingFaceEmbeddings'>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:58:03.375822Z",
     "start_time": "2024-05-01T05:58:00.168957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Use vector store to store embeddings\\\n",
    "with JobTimer(\"4.3.1 - LangChain [4/4]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    document_vector = FAISS.from_documents(all_splits, model_embeddings)\n",
    "md(f\"{type(document_vector)}\")"
   ],
   "id": "d638881dea441cc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:58:00] ┇ ================================================================================\n",
      "[05.01 14:58:00] ┇ [INIT] 4.3.1 - LangChain [4/4]\n",
      "[05.01 14:58:00] ┇ ================================================================================\n",
      "[05.01 14:58:02] ┇ Loading faiss.\n",
      "[05.01 14:58:02] ┇ Successfully loaded faiss.\n",
      "[05.01 14:58:03] ┇ ================================================================================\n",
      "[05.01 14:58:03] ┇ [EXIT] 4.3.1 - LangChain [4/4] ($=00:00:02.372)\n",
      "[05.01 14:58:03] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<class 'langchain_community.vectorstores.faiss.FAISS'>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **4.3.2 - LangChain Q&A Retriever**\n",
    "* ConversationalRetrievalChain\n",
    "* Query the Source documents"
   ],
   "id": "a3d7e1cf197ad834"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:58:03.384025Z",
     "start_time": "2024-05-01T05:58:03.377563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the Llama 2 model hosted on Replicate\n",
    "# Temperature: Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic, 0.75 is a good starting value\n",
    "# top_p: When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens\n",
    "# max_new_tokens: Maximum number of tokens to generate. A word is generally 2-3 tokens\n",
    "llm_ref = Replicate(\n",
    "    model=model_ref,\n",
    "    model_kwargs={\"temperature\": 0.75, \"top_p\": 1, \"max_new_tokens\": 1000}\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm_ref, document_vector.as_retriever(), return_source_documents=True)\n",
    "md(f\"{type(chain)}\")"
   ],
   "id": "bfad7ef283df7b39",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<class 'langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain'>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:58:08.803280Z",
     "start_time": "2024-05-01T05:58:03.385093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Query against your own data\n",
    "chat_history = []\n",
    "query = \"How is Meta approaching open science in two short sentences?\"\n",
    "with JobTimer(\"4.3.2 - LangChain Q&A Retriever [1/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "html(result['answer'])"
   ],
   "id": "8b8a69f63c4b1b08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:58:03] ┇ ================================================================================\n",
      "[05.01 14:58:03] ┇ [INIT] 4.3.2 - LangChain Q&A Retriever [1/2]\n",
      "[05.01 14:58:03] ┇ ================================================================================\n",
      "[05.01 14:58:04] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:05] ┇ HTTP Request: GET https://api.replicate.com/v1/models/meta/llama-2-13b-chat/versions/f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:06] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:58:07] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/ytvs3mt8xhrgp0cf6axswy1jd4 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:08] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/ytvs3mt8xhrgp0cf6axswy1jd4 \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:08] ┇ ================================================================================\n",
      "[05.01 14:58:08] ┇ [EXIT] 4.3.2 - LangChain Q&A Retriever [1/2] ($=00:00:04.580)\n",
      "[05.01 14:58:08] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Sure, I'd be happy to help! Here's my answer based on the provided context:\n",
       "\n",
       "Meta is approaching open science by democratizing access and collaboration on risk management, and by empowering developers in every industry on a global scale to drive breakthroughs and create new products and solutions. This includes open sourcing code and datasets, supporting the AI-developer community with tools like PyTorch, ONNX, Glow, and Detectron, and making cutting-edge large language models (LLMs) available to the scientific community through research releases.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:58:16.727161Z",
     "start_time": "2024-05-01T05:58:08.804282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This time your previous question and answer will be included as a chat history which will enable the ability\n",
    "# to ask follow up questions.\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"How is it benefiting the world?\"\n",
    "with JobTimer(\"4.3.2 - LangChain Q&A Retriever [2/2]\", rt=1, rb=1, rw=80, rc='=', verbose=1):\n",
    "    result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "html(result['answer'])"
   ],
   "id": "d7741a0aa701af69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05.01 14:58:09] ┇ ================================================================================\n",
      "[05.01 14:58:09] ┇ [INIT] 4.3.2 - LangChain Q&A Retriever [2/2]\n",
      "[05.01 14:58:09] ┇ ================================================================================\n",
      "[05.01 14:58:09] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:58:10] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/r4tq3ajq99rgj0cf6axrfztn7c \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:11] ┇ HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "[05.01 14:58:11] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/t735w02x2drgg0cf6axr7jynvr \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:12] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/t735w02x2drgg0cf6axr7jynvr \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:13] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/t735w02x2drgg0cf6axr7jynvr \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:14] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/t735w02x2drgg0cf6axr7jynvr \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:15] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/t735w02x2drgg0cf6axr7jynvr \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:16] ┇ HTTP Request: GET https://api.replicate.com/v1/predictions/t735w02x2drgg0cf6axr7jynvr \"HTTP/1.1 200 OK\"\n",
      "[05.01 14:58:16] ┇ ================================================================================\n",
      "[05.01 14:58:16] ┇ [EXIT] 4.3.2 - LangChain Q&A Retriever [2/2] ($=00:00:07.082)\n",
      "[05.01 14:58:16] ┇ ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='font-family:Arial; font-size:12pt; padding:10px; margin-left:10px; margin-top:10px; background-color:LightSkyBlue; border:3px solid MidnightBlue;'> Based on the provided context, here's a helpful answer to the standalone question:\n",
       "\n",
       "Meta's approach to open science provides several benefits to the world, including:\n",
       "\n",
       "1. Democratization of access: By open-sourcing code and datasets, Meta aims to make AI development more accessible to developers globally, regardless of their background or location.\n",
       "2. Collaboration on risk management: By encouraging collaboration and open communication, Meta hopes to address global challenges more effectively and responsibly.\n",
       "3. Acceleration of technological advancement: By empowering developers to drive breakthroughs and create new products, Meta's approach to open science can lead to faster technological progress.\n",
       "4. Economic growth: By unlocking the power of AI, Meta's approach to open science can potentially revolutionize various sectors, leading to economic growth and development.\n",
       "5. Addressing global challenges: Meta believes that the power of AI can be harnessed to address global challenges such as education, agriculture, climate management, and cybersecurity.\n",
       "6. Unlocking the potential of AI: By making AI development more accessible and collaborative, Meta's approach to open science can help unlock the potential of AI to drive breakthroughs and innovation.\n",
       "\n",
       "Overall, Meta's approach to open science aims to create a vibrant AI-innovation ecosystem that can address global challenges, unlock the potential of AI, and drive technological advancement and economic growth.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **5 - Fine-Tuning Models**\n",
    "* Limitatons of Prompt Eng and RAG\n",
    "* Fine-Tuning Arch\n",
    "* Types (PEFT, LoRA, QLoRA)\n",
    "* Using PyTorch for Pre-Training & Fine-Tuning\n",
    "* Evals + Quality"
   ],
   "id": "485ed0b25f0baab6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T05:58:16.730853Z",
     "start_time": "2024-05-01T05:58:16.728085Z"
    }
   },
   "cell_type": "code",
   "source": "fine_tuned_arch()",
   "id": "44df90baa5f9d992",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBncmFwaCBMUjsKICAgICAgICBDdXN0b21fRGF0YXNldCAtLT4gUHJlLXRyYWluZWRfTGxhbWEKICAgICAgICBQcmUtdHJhaW5lZF9MbGFtYSAtLT4gRmluZS10dW5lZF9MbGFtYQogICAgICAgIEZpbmUtdHVuZWRfTGxhbWEgLS0+IFJMSEYKICAgICAgICBSTEhGIC0tPiB8TG9zczpDcm9zcy1FbnRyb3B5fEZpbmUtdHVuZWRfTGxhbWEKICAgICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICAgIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6 - Responsible AI**\n",
    "* Power + Responsibility\n",
    "* Hallucinations\n",
    "* Input & Output Safety\n",
    "* Red-teaming (simulating real-world cyber attackers)\n",
    "* [Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/)"
   ],
   "id": "20d9ee189c8ef88d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **7 - Conclusion**\n",
    "* Active research on LLMs and Llama\n",
    "* Leverage the power of Llama and its open community\n",
    "* Safety and responsible use is paramount!\n",
    "* Call-To-Action\n",
    "  * [Replicate Free Credits](https://replicate.fyi/connect2023) for Connect attendees!\n",
    "  * This notebook is available through Llama Github recipes\n",
    "  * Use Llama in your projects and give us feedback"
   ],
   "id": "8e65302a917a841b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Resources**\n",
    "- [GitHub - Llama 2](https://github.com/facebookresearch/llama)\n",
    "- [Github - LLama 2 Recipes](https://github.com/facebookresearch/llama-recipes)\n",
    "- [Llama 2](https://ai.meta.com/llama/)\n",
    "- [Research Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "- [Model Card](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)\n",
    "- [Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/)\n",
    "- [Acceptable Use Policy](https://ai.meta.com/llama/use-policy/)\n",
    "- [Replicate](https://replicate.com/meta/)\n",
    "- [LangChain](https://www.langchain.com/)"
   ],
   "id": "d201a9aa70cfcad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Authors & Contact**\n",
    "  * asangani@meta.com, [Amit Sangani | LinkedIn](https://www.linkedin.com/in/amitsangani/)\n",
    "  * mohsena@meta.com, [Mohsen Agsen | LinkedIn](https://www.linkedin.com/in/mohsen-agsen-62a9791/)\n"
   ],
   "id": "bf0012709004082a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
